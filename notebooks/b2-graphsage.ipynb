{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yS3hhwYyt2wP"
   },
   "source": [
    "# Using GraphSAGE to Generate Embeddings for Unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v9Mownut2wS"
   },
   "source": [
    "The GraphSAGE (SAmple and aggreGatE) algorithm [13] emerged in 2017 as a method for not only learning useful vertex embeddings, but also for predicting vertex embeddings on unseen vertices. This allows powerful high-level feature vectors to be produced for vertices which were not seen at train time; enabling us to effectively work with dynamic graphs, or very large graphs (>100, 000 vertices).\n",
    "\n",
    "\n",
    "A GraphSAGE net is built up of k convolutional layers, called SageConv layers by the authors. Like other GNNs, they use a message-passing algorithm to combine neighbourhood features for each node. These features are then aggregated using a reduce function like max pool or mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AW2N3UAt2wT"
   },
   "source": [
    "## Setup\n",
    "Here we load required libraries, define paths to data, and define some helper functions. **Feel free to skip this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc14Z5j2t2wV",
    "outputId": "b584a0cc-63a9-41b0-84e6-ecdd5b961904"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  !pip install dgl\n",
    "else:\n",
    "  !pip install dgl-cu110\n",
    "\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn2ZkwoAt2wX"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rogq5WIOt2wX"
   },
   "source": [
    "In this example we use the Cora dataset (see Figure 19) as provided by the deep learning library DGL.\n",
    "\n",
    "The Cora dataset is oft considered ‘the MNIST of graph-based learning’ and consists of 2708 scientific publications (vertices), each classified into one of seven subfields in AI (or classes). Each vertex has a 1433 element binary feature vector, which indicates if each of the 1433 designated words appeared in the publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oz1xxUomt2wY",
    "outputId": "35f5c15d-7032-4201-bac5-13730b6d3bad"
   },
   "outputs": [],
   "source": [
    "# To demonstrate let's use the Cora dataset\n",
    "# DGL provides an api to access this and other datasets.\n",
    "import dgl.data \n",
    "data = dgl.data.CoraGraphDataset()\n",
    "print('Number of categories:', data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "Gh6Hy7Yzt2wZ",
    "outputId": "8a38b338-e0b8-494e-c93d-eda1f868d324"
   },
   "outputs": [],
   "source": [
    "# DGL datasets come with a pre-initialised graph\n",
    "g = data[0]\n",
    "features = g.ndata['feat']\n",
    "# that will download and cache the data for use later\n",
    "# let's investigate\n",
    "n_features = int(features.shape[1])\n",
    "n_nodes = int(features.shape[0])\n",
    "n_edges = g.number_of_edges()\n",
    "\n",
    "print(f'There are {n_nodes} nodes and {n_edges} edges')\n",
    "print(f'Each node has {n_features} features')\n",
    "\n",
    "# # let's look at the labels, the classification target\n",
    "labels = g.ndata['label']\n",
    "n_classes = labels.max() + 1\n",
    "print(f\"There are {n_classes} classes\")\n",
    "fig, ax = plt.subplots()\n",
    "n, bins, patches = ax.hist(labels.flatten()[:20000], bins=n_classes, density=True)\n",
    "plt.show()\n",
    "# plt.hist(labels.flatten()[:20000], bins=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBky3N1Xt2wZ",
    "outputId": "3f579495-49f5-48b7-a2c5-b27e370d7511"
   },
   "outputs": [],
   "source": [
    "# DGL datasets come preprepared with train/test/val splits, in the form of index masks\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "print(int(train_mask.sum()), 'train samples')\n",
    "print(int(val_mask.sum()), 'validation samples')\n",
    "print(int(test_mask.sum()), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpb5ggQht2wa"
   },
   "outputs": [],
   "source": [
    "# Let's convert the data from numpy arrays to the required pytorch tensors. \n",
    "features = torch.FloatTensor(features)\n",
    "labels = torch.LongTensor(labels)\n",
    "train_mask = torch.BoolTensor(train_mask)\n",
    "val_mask = torch.BoolTensor(val_mask)\n",
    "test_mask = torch.BoolTensor(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwHJLB6Ht2wa",
    "outputId": "17d75ce8-91fc-471c-93bb-c6ab9a61bbd2"
   },
   "outputs": [],
   "source": [
    "# If we are using the gpu, we can send the arrays to gpu memory.\n",
    "print(torch.cuda.is_available())\n",
    "gpu = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    gpu = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(gpu)\n",
    "    features = features.cuda()\n",
    "    labels = labels.cuda()\n",
    "    train_mask = train_mask.cuda()\n",
    "    val_mask = val_mask.cuda()\n",
    "    test_mask = test_mask.cuda()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52L1anFLt2wb"
   },
   "source": [
    "A subgraph of the Cora dataset. The full Cora graph has N = 2708 and M = 5429. Note the many vertices with few incident edges (low degree) as compared to the few vertices with many incident edges (high degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "3sXHn9mDt2wb",
    "outputId": "c53a386c-b106-4165-f364-2772c48b7b34",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DGL datasets come with a pre-initialised networkx graph\n",
    "gx = data.graph\n",
    "\n",
    "# first remove any existing self-loops, because graphSAGE employs\n",
    "# its own way of dealing with self-loops in the forward pass\n",
    "gx.remove_edges_from(nx.selfloop_edges(gx))\n",
    "# and lets recalc the num of edges for later\n",
    "n_edges = gx.number_of_edges()\n",
    "# for simplicity lets convert the graph to an undirected one\n",
    "gx = gx.to_undirected()\n",
    "\n",
    "# with a networkx graph we can do some plotting\n",
    "# lets just plot a fraction of the nodes\n",
    "gx_copy = gx.copy()\n",
    "gx_copy.remove_nodes_from(range(500, n_nodes))\n",
    "nx.draw(gx_copy, node_size=10, alpha=0.6, arrows=False, edge_color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myCYVbPdt2wc"
   },
   "outputs": [],
   "source": [
    "# We can build a trainable GNN out of this networkx graph with dgl/\n",
    "# The DGLGraph can take a networkx graph as input\n",
    "g = dgl.from_networkx(gx)\n",
    "if torch.cuda.is_available():\n",
    "  g = g.to(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dpb15Bdt2wc"
   },
   "source": [
    "## Architecture and initial experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBgkxV_5t2wd"
   },
   "source": [
    "We'll start by setting up our own layers, models, and training routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3ZcSHdQt2wd"
   },
   "outputs": [],
   "source": [
    "# Like all layers and neural nets in pytorch we will inherit the Module class\n",
    "class MeanAggSageLayer(nn.Module):\n",
    "    def __init__(self, n_features_in, n_features_out):\n",
    "        super(MeanAggSageLayer, self).__init__()\n",
    "        # number of features coming in to this layer. If this is the first layer, \n",
    "        # this will be the amount of features per node\n",
    "        self._in = n_features_in\n",
    "        # the number of output features from this layer,\n",
    "        # In the final layer of the GraphSAGE net this will equal n_classes \n",
    "        self._out = n_features_out\n",
    "        # create a linear transformation between the input channels and the output.\n",
    "        # These nn.Linear objects are shortcuts to hold the weights and biases\n",
    "        # that are learnt through backpropogation, and applied\n",
    "        # to incoming features. We will have one for self nodes \n",
    "        self.fc_self = nn.Linear(self._in, self._out)\n",
    "        # and one for neighbour nodes \n",
    "        self.fc_neigh = nn.Linear(self._in, self._out)\n",
    "        # we will initialise the weights with xavier_unform random\n",
    "        # sampling, another name for Glorot uniform used in the original\n",
    "        # graphsage paper\n",
    "        gain = nn.init.calculate_gain('relu')   # sqrt(2)\n",
    "        # set the gain appropriately for our activation function \n",
    "        nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
    "    \n",
    "    def forward(self, graph, features):\n",
    "        \"\"\"\n",
    "        The following code is DGL's way of using the graph class\n",
    "        to facilitate message passing. The equivalent code in pure pytorch\n",
    "        operating instead on the adjacency matrix adj and the feature matrix x would be:\n",
    "            \n",
    "            def forward(self, x, adj):\n",
    "                return adj.matmul(x, reduce='mean') @ self_weights + x @ neigh_weights + bias\n",
    "        \n",
    "        \"\"\"\n",
    "        # set the incoming features matrix as the input to this layer 'h'\n",
    "        graph.srcdata['h'] = features\n",
    "        # create 2 user defined functions, the first to collect features \n",
    "        # from the src nodes 'h', send along edges 'm', and aggregate them at the \n",
    "        # destination nodes (the neighbours)\n",
    "        features_from_src_nodes = dgl.function.copy_src('h', 'm')\n",
    "        aggregation_at_dst_nodes = dgl.function.mean('m', 'neigh')\n",
    "        # graph.update_all is a helper function to send the first function\n",
    "        # along the edges and recieve the second function at the\n",
    "        # destination nodes\n",
    "        graph.update_all(features_from_src_nodes, aggregation_at_dst_nodes)\n",
    "        # now we can get our aggregated neighbourhood features\n",
    "        h_neigh = graph.dstdata['neigh']\n",
    "        # and combine them with the src features (self loops)\n",
    "        # fc_self(features) is equivalent to features @ weights + biases\n",
    "        output = self.fc_self(features) + self.fc_neigh(h_neigh)\n",
    "        # lastly we add a nonlinearity to the output enabling backpropogation\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXaVbcWDt2wg"
   },
   "source": [
    "The only method we need will be the 'self.forward' method (forward pass). The backpropogation will be handled by the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0X6r4mnt2wg"
   },
   "source": [
    "Now lets build a graphSAGE GNN out of these layers that takes in a DGLGraph we made previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UHdPyYIt2wh"
   },
   "outputs": [],
   "source": [
    "class SimpleGraphSAGE(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            g, \n",
    "            n_features, \n",
    "            n_hidden, \n",
    "            n_classes, \n",
    "            n_layers\n",
    "    ):\n",
    "        super(SimpleGraphSAGE, self).__init__()\n",
    "        # A ModuleList will hold all of our layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.g = g\n",
    "\n",
    "        # input layer, the input size of which will be \n",
    "        # the number of features\n",
    "        self.conv_layers.append(MeanAggSageLayer(n_features, n_hidden))\n",
    "        # create the hidden layers: (n_layers - 1) allowing for the output layer\n",
    "        for i in range(n_layers - 1):\n",
    "            self.conv_layers.append(MeanAggSageLayer(n_hidden, n_hidden))\n",
    "        # output layer, the output size of which will be the number of classes\n",
    "        self.conv_layers.append(MeanAggSageLayer(n_hidden, n_classes))\n",
    "\n",
    "    def forward(self, features):\n",
    "        # h(0) will be equal to the feature matrix\n",
    "        h = features\n",
    "        for conv in self.conv_layers:\n",
    "            # pass h through one layer and back into the next\n",
    "            h = conv(self.g, h)\n",
    "        # now we have h(k)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDGdK2-jt2wh"
   },
   "source": [
    "Before we create one of these models we need to decide on some params:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfBImZP1t2wh"
   },
   "outputs": [],
   "source": [
    "n_hidden = 16\n",
    "n_layers = 2\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.0005\n",
    "n_epochs = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOsLIP2at2wh"
   },
   "source": [
    "Now we can create a GraphSAGE model using our graph (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rdhSHw9t2wi"
   },
   "outputs": [],
   "source": [
    "model = SimpleGraphSAGE(g, n_features, n_hidden, n_classes, n_layers)\n",
    "# we can send this to gpu memory as well\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF6xpKktt2wi"
   },
   "outputs": [],
   "source": [
    "# use cross entropy loss function\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j13azUCOt2wj"
   },
   "outputs": [],
   "source": [
    "# we also need a scoring function, lets create a simple accuracy calculator:\n",
    "def get_accuracy(pred, true):\n",
    "    _, indices = torch.max(pred, dim=1)\n",
    "    correct = torch.sum(indices == true)\n",
    "    return correct.item() * 1.0 / len(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSr3wugRt2wj"
   },
   "source": [
    "And we can decide on a simple training routine too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Uzrtvv-t2wk"
   },
   "outputs": [],
   "source": [
    "# now our training pipeline is able to be built\n",
    "def train(model, optimizer, n_epochs):\n",
    "    # we will keep track how long each epoch takes so we can calculate things like\n",
    "    # Traversed Edges Per Second (TEPS)\n",
    "    dur = []\n",
    "    all_train_logits = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # This doesnt train the mdoel, instead it tells all the child modules\n",
    "        # that the model is in training mode and not evaluating mode\n",
    "        # (for examplee, when evaluating, you dont want to apply dropout to the input tensor)\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "\n",
    "        # the forward pass - sending the features to the model.forward method\n",
    "        output = model(features)\n",
    "        # calculate our current loss by comparing only the training nodes'\n",
    "        # prediction and truth\n",
    "        output_train = output[train_mask]\n",
    "        loss = loss_fcn(output_train, labels[train_mask])\n",
    "\n",
    "        # the backwards pass! update the weights in our SAGELayers - but first:\n",
    "        # reset the gradient back to 0 before doing backpropogation\n",
    "        # (pytorch by default accumulates the gradients after each backward pass)\n",
    "        optimizer.zero_grad()\n",
    "        # backpropogation\n",
    "        loss.backward()\n",
    "        # step the adam optimizer forward\n",
    "        optimizer.step()\n",
    "\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "        # set the model into evaluation model\n",
    "        model.eval()\n",
    "        # temporatily turn off the gradient calculation as\n",
    "        # just want to simply inference\n",
    "        with torch.no_grad():\n",
    "            output_val = output[val_mask]\n",
    "            labels_val = labels[val_mask]\n",
    "            acc = get_accuracy(output_val, labels_val)\n",
    "\n",
    "        # record the output logits for plotting later\n",
    "        all_train_logits.append(output_train)\n",
    "\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "              \"TEPS {:.2f}\".format(epoch, np.mean(dur), loss.item(), acc, \n",
    "                                   n_edges / np.mean(dur)))\n",
    "\n",
    "    print('training complete')\n",
    "    return model, output, all_train_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9qTstv_t2wk",
    "outputId": "d26d74e0-59eb-4640-d9aa-0eefbef4754b"
   },
   "outputs": [],
   "source": [
    "model, last_output, _ = train(model, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDebhJlnt2wk",
    "outputId": "d3e76e4f-26aa-48ec-810b-b30e5a78ba6a"
   },
   "outputs": [],
   "source": [
    "# now we can evaluate the model on the test set\n",
    "output_test = last_output[test_mask]\n",
    "labels_test = labels[test_mask]\n",
    "acc = get_accuracy(output_test, labels_test)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-SnGxmt2wl"
   },
   "source": [
    "## Further experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46pEiaQ8t2wl"
   },
   "source": [
    "Ok so not too impressive - how can we improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFOh5VNvt2wl"
   },
   "source": [
    "For one, there are other aggregation methods used in the original paper.\n",
    "DGL has implemented a SAGEConv layer that takes our simplified SageLayer further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H1hH-Ikt2wm"
   },
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch.conv.sageconv import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_nW2rott2wm"
   },
   "outputs": [],
   "source": [
    "# A new graphSAGE net could be built as follows:\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE pytorch implementation from paper `Inductive Representation Learning on\n",
    "    Large Graphs <https://arxiv.org/pdf/1706.02216.pdf>`__.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            g,\n",
    "            n_features,\n",
    "            n_hidden,\n",
    "            n_classes,\n",
    "            n_layers,\n",
    "            agg,\n",
    "            activation,\n",
    "            dropout,\n",
    "    ):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.g = g\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(\n",
    "            SAGEConv(n_features, n_hidden, agg, feat_drop=dropout, activation=activation)\n",
    "        )\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(\n",
    "                SAGEConv(n_hidden, n_hidden, agg, feat_drop=dropout, activation=activation)\n",
    "            )\n",
    "        # output layer\n",
    "        self.layers.append(\n",
    "            SAGEConv(n_hidden, n_classes, agg, feat_drop=dropout, activation=None)\n",
    "        ) # no activation None for final layer\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(self.g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itDo74MHt2wm"
   },
   "source": [
    "The 'agg' variable can now be one of ['mean', 'gcn', 'pool', 'lstm'].\n",
    "Additionally, a dropout fraction can be set, activation can be changed from 'relu', and the SAGEConv layer also supports an optional normalization function.\n",
    "\n",
    "We'll start by looking at the **mean** aggregation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k63Ao5E3t2wm"
   },
   "outputs": [],
   "source": [
    "# lets try our same params as before but using a dropout value of 0.5\n",
    "model = GraphSAGE(g, n_features, n_hidden, n_classes, n_layers, 'mean', F.relu, 0.5)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# initialize the optimzier again as the model params have changed\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwN6wPbWt2wn",
    "outputId": "717163eb-a61b-492e-9a93-f9a0e5241d27"
   },
   "outputs": [],
   "source": [
    "model, last_output, all_train_logits = train(model, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yTTbNBFt2wn",
    "outputId": "5e48e4b4-6408-4aef-f7e2-d63857b28d23"
   },
   "outputs": [],
   "source": [
    "acc = get_accuracy(last_output[test_mask], labels_test)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80-x2gnCt2wn"
   },
   "source": [
    "Slightly better! Lets change the aggregation function. \n",
    "in the original GraphSAGE paper they found the LSTM and pool methods generally outperformed the mean and GCN aggregation across a range of datasets.\n",
    "Lets try the **pool** method (which refers to a max pool aggregator over the neighbourhood) and bump the number of hidden channels up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mhJL9KKt2wo"
   },
   "outputs": [],
   "source": [
    "model = GraphSAGE(g, n_features, 128, n_classes, 2, 'pool', F.relu, 0.3)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.003, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-Qj8gBCt2wo",
    "outputId": "f59c5247-8925-46f3-cd07-29e0709879cd"
   },
   "outputs": [],
   "source": [
    "model, last_output, all_train_logits = train(model, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJg8VMvct2wp",
    "outputId": "aa2ee32b-4462-4abc-c392-142358e3fbfa"
   },
   "outputs": [],
   "source": [
    "acc = get_accuracy(last_output[test_mask], labels_test)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bArleowDt2wq"
   },
   "source": [
    "And finally the **LSTM** aggregation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQXlnGimt2wq"
   },
   "outputs": [],
   "source": [
    "model = GraphSAGE(g, n_features, 128, n_classes, 2, 'lstm', F.relu, 0.1)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.003, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXPwp6JGt2wq",
    "outputId": "347e7d58-35b9-4dc5-d320-587bcc1f99d9"
   },
   "outputs": [],
   "source": [
    "model, last_output, all_train_logits = train(model, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoOwF4Ogt2wq",
    "outputId": "58451e69-08a2-4cc4-fdda-e4a4645784a1"
   },
   "outputs": [],
   "source": [
    "acc = get_accuracy(last_output[test_mask], labels_test)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBiQ3rrtt2wr"
   },
   "source": [
    "Not bad! See how high you can get the accuracy with some tweaking. \n",
    "Compare against the state-of-the-art here: https://paperswithcode.com/sota/node-classification-on-cora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te521FPqt2wr"
   },
   "source": [
    "We can plot an animation of the predictions during training (although we are limited to 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QNhB3OUt2wr"
   },
   "outputs": [],
   "source": [
    "# one colour for each class\n",
    "colors = ['red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink']\n",
    "# to keep the graph small lets only consider training nodes\n",
    "train_nodes = train_mask.cpu().numpy()\n",
    "labels_train = labels[train_mask]\n",
    "non_train = np.ones(len(train_nodes))\n",
    "non_train[train_nodes] = 0\n",
    "non_train = np.where(non_train)[0]\n",
    "if torch.cuda.is_available():\n",
    "  nx_g = model.g.cpu().to_networkx()\n",
    "else:\n",
    "  nx_g = model.g.to_networkx()\n",
    "nx_g.remove_nodes_from(non_train)\n",
    "rn_nodes = range(nx_g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3C5bvfft2wr"
   },
   "outputs": [],
   "source": [
    "def draw_epoch(i):\n",
    "    current_colors = []\n",
    "    if torch.cuda.is_available():\n",
    "        logits = all_train_logits[i].detach().cpu().numpy()\n",
    "    else:\n",
    "        logits = all_train_logits[i].detach().numpy()\n",
    "        \n",
    "    max_ix = logits.argmax(axis=1)\n",
    "    \n",
    "    # choose x, y position based on the magnitude of their highest \n",
    "    #min_ix = max_ix - 1\n",
    "    #pos = {n: [logits[n, max_ix[n]], logits[n, min_ix[n]]] for n in rn_nodes}\n",
    "    #node_size = 100\n",
    "    # x=node_index, y = certainty, color=class \n",
    "    #pos = {n: [n, logits[n, max_ix[n]]] for n in rn_nodes}\n",
    "    #node_size = 100\n",
    "    \n",
    "    # x=node_index, y = class, size = certainty\n",
    "    pos = {n: [n, max_ix[n]] for n in rn_nodes}\n",
    "    node_size = logits.max(axis=1) * 100\n",
    "    \n",
    "    # cols = [colors[max_ix[n]] for n in rn_nodes]\n",
    "    # use real label for color    \n",
    "    cols = [colors[labels_train[n]] for n in rn_nodes]\n",
    "    \n",
    "    ax.cla()\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Epoch: %d' % i)\n",
    "    nx.draw_networkx(nx_g, pos, node_color=cols,\n",
    "            with_labels=True, node_size=node_size, ax=ax, \n",
    "            edge_color='purple', arrows=False, alpha=0.6)\n",
    "\n",
    "fig = plt.figure(dpi=100)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "draw_epoch(0)  # draw the prediction of the first epoch\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s0o1A8xt2ws"
   },
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, draw_epoch, frames=len(all_train_logits), interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "ngxlIRrpt2ws",
    "outputId": "890090a3-2215-454b-c90e-7966a901fb4d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo2w5BqPt2xI"
   },
   "source": [
    "Note how the separation of the nodes into classes improves with more training epochs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "b2-graphsage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gnn_tutorial",
   "language": "python",
   "name": "gnn_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
