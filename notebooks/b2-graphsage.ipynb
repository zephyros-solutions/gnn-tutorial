{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "b2-graphsage.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS3hhwYyt2wP"
      },
      "source": [
        "# Using GraphSAGE to Generate Embeddings for Unseen Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v9Mownut2wS"
      },
      "source": [
        "The GraphSAGE (SAmple and aggreGatE) algorithm [13] emerged in 2017 as a method for not only learning useful vertex embeddings, but also for predicting vertex embeddings on unseen vertices. This allows powerful high-level feature vectors to be produced for vertices which were not seen at train time; enabling us to effectively work with dynamic graphs, or very large graphs (>100, 000 vertices).\n",
        "\n",
        "\n",
        "A GraphSAGE net is built up of k convolutional layers, called SageConv layers by the authors. Like other GNNs, they use a message-passing algorithm to combine neighbourhood features for each node. These features are then aggregated using a reduce function like max pool or mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AW2N3UAt2wT"
      },
      "source": [
        "## Setup\n",
        "Here we load required libraries, define paths to data, and define some helper functions. **Feel free to skip this section.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc14Z5j2t2wV",
        "outputId": "828bc4da-c28f-402e-ca59-9555eb09bc90"
      },
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "from IPython.display import HTML\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  !pip install dgl\n",
        "else:\n",
        "  !pip install dgl-cu110\n",
        "\n",
        "import dgl\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl-cu110\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/e5/8536dc2f7048a91c63c913ece8c7b2e7f3e438e741aa4c2d847395486617/dgl_cu110-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (39.9MB)\n",
            "\u001b[K     |████████████████████████████████| 39.9MB 68kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu110) (1.19.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu110) (2.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu110) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu110) (1.4.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl-cu110) (4.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu110) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu110) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu110) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu110) (1.24.3)\n",
            "Installing collected packages: dgl-cu110\n",
            "Successfully installed dgl-cu110-0.6.1\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2ZkwoAt2wX"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rogq5WIOt2wX"
      },
      "source": [
        "In this example we use the Cora dataset (see Figure 19) as provided by the deep learning library DGL.\n",
        "\n",
        "The Cora dataset is oft considered ‘the MNIST of graph-based learning’ and consists of 2708 scientific publications (vertices), each classified into one of seven subfields in AI (or classes). Each vertex has a 1433 element binary feature vector, which indicates if each of the 1433 designated words appeared in the publication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz1xxUomt2wY",
        "outputId": "709a51a8-6401-4b0c-9fef-2d19fe6dd38e"
      },
      "source": [
        "# To demonstrate let's use the Cora dataset\n",
        "# DGL provides an api to access this and other datasets.\n",
        "import dgl.data \n",
        "data = dgl.data.CoraGraphDataset()\n",
        "print('Number of categories:', data.num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "Number of categories: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Gh6Hy7Yzt2wZ",
        "outputId": "8dee675e-2231-4bb8-c6e2-608b242e4260"
      },
      "source": [
        "# DGL datasets come with a pre-initialised graph\n",
        "g = data[0]\n",
        "features = g.ndata['feat']\n",
        "# that will download and cache the data for use later\n",
        "# let's investigate\n",
        "n_features = int(features.shape[1])\n",
        "n_nodes = int(features.shape[0])\n",
        "n_edges = g.number_of_edges()\n",
        "\n",
        "print(f'There are {n_nodes} nodes and {n_edges} edges')\n",
        "print(f'Each node has {n_features} features')\n",
        "\n",
        "# # let's look at the labels, the classification target\n",
        "labels = g.ndata['label']\n",
        "n_classes = labels.max() + 1\n",
        "print(f\"There are {n_classes} classes\")\n",
        "fig, ax = plt.subplots()\n",
        "n, bins, patches = ax.hist(labels.flatten()[:20000], bins=n_classes, density=True)\n",
        "plt.show()\n",
        "# plt.hist(labels.flatten()[:20000], bins=n_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2708 nodes and 10556 edges\n",
            "Each node has 1433 features\n",
            "There are 7 classes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBky3N1Xt2wZ",
        "outputId": "cd661f44-721c-443a-f264-9ca2f37baeaa"
      },
      "source": [
        "# DGL datasets come preprepared with train/test/val splits, in the form of index masks\n",
        "train_mask = g.ndata['train_mask']\n",
        "val_mask = g.ndata['val_mask']\n",
        "test_mask = g.ndata['test_mask']\n",
        "print(int(train_mask.sum()), 'train samples')\n",
        "print(int(val_mask.sum()), 'validation samples')\n",
        "print(int(test_mask.sum()), 'test samples')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140 train samples\n",
            "500 validation samples\n",
            "1000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpb5ggQht2wa"
      },
      "source": [
        "# Let's convert the data from numpy arrays to the required pytorch tensors. \n",
        "features = torch.FloatTensor(features)\n",
        "labels = torch.LongTensor(labels)\n",
        "train_mask = torch.BoolTensor(train_mask)\n",
        "val_mask = torch.BoolTensor(val_mask)\n",
        "test_mask = torch.BoolTensor(test_mask)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwHJLB6Ht2wa",
        "outputId": "c47093ce-2cbc-4bd1-8f68-737b4a98fd69"
      },
      "source": [
        "# If we are using the gpu, we can send the arrays to gpu memory.\n",
        "print(torch.cuda.is_available())\n",
        "gpu = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    gpu = torch.device('cuda:0')\n",
        "    torch.cuda.set_device(gpu)\n",
        "    features = features.cuda()\n",
        "    labels = labels.cuda()\n",
        "    train_mask = train_mask.cuda()\n",
        "    val_mask = val_mask.cuda()\n",
        "    test_mask = test_mask.cuda()   "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52L1anFLt2wb"
      },
      "source": [
        "A subgraph of the Cora dataset. The full Cora graph has N = 2708 and M = 5429. Note the many vertices with few incident edges (low degree) as compared to the few vertices with many incident edges (high degree)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "3sXHn9mDt2wb",
        "scrolled": true,
        "outputId": "81f11d73-0c9b-4eb5-9a43-2c1cc2b20ba0"
      },
      "source": [
        "# DGL datasets come with a pre-initialised networkx graph\n",
        "gx = data.graph\n",
        "\n",
        "# first remove any existing self-loops, because graphSAGE employs\n",
        "# its own way of dealing with self-loops in the forward pass\n",
        "gx.remove_edges_from(nx.selfloop_edges(gx))\n",
        "# and lets recalc the num of edges for later\n",
        "n_edges = gx.number_of_edges()\n",
        "# for simplicity lets convert the graph to an undirected one\n",
        "gx = gx.to_undirected()\n",
        "\n",
        "# with a networkx graph we can do some plotting\n",
        "# lets just plot a fraction of the nodes\n",
        "gx_copy = gx.copy()\n",
        "gx_copy.remove_nodes_from(range(500, n_nodes))\n",
        "nx.draw(gx_copy, node_size=10, alpha=0.6, arrows=False, edge_color='purple')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.\n",
            "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myCYVbPdt2wc"
      },
      "source": [
        "# We can build a trainable GNN out of this networkx graph with dgl/\n",
        "# The DGLGraph can take a networkx graph as input\n",
        "g = dgl.from_networkx(gx)\n",
        "if torch.cuda.is_available():\n",
        "  g = g.to(gpu)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dpb15Bdt2wc"
      },
      "source": [
        "## Architecture and initial experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBgkxV_5t2wd"
      },
      "source": [
        "We'll start by setting up our own layers, models, and training routines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ZcSHdQt2wd"
      },
      "source": [
        "# Like all layers and neural nets in pytorch we will inherit the Module class\n",
        "class MeanAggSageLayer(nn.Module):\n",
        "    def __init__(self, n_features_in, n_features_out):\n",
        "        super(MeanAggSageLayer, self).__init__()\n",
        "        # number of features coming in to this layer. If this is the first layer, \n",
        "        # this will be the amount of features per node\n",
        "        self._in = n_features_in\n",
        "        # the number of output features from this layer,\n",
        "        # In the final layer of the GraphSAGE net this will equal n_classes \n",
        "        self._out = n_features_out\n",
        "        # create a linear transformation between the input channels and the output.\n",
        "        # These nn.Linear objects are shortcuts to hold the weights and biases\n",
        "        # that are learnt through backpropogation, and applied\n",
        "        # to incoming features. We will have one for self nodes \n",
        "        self.fc_self = nn.Linear(self._in, self._out)\n",
        "        # and one for neighbour nodes \n",
        "        self.fc_neigh = nn.Linear(self._in, self._out)\n",
        "        # we will initialise the weights with xavier_unform random\n",
        "        # sampling, another name for Glorot uniform used in the original\n",
        "        # graphsage paper\n",
        "        gain = nn.init.calculate_gain('relu')   # sqrt(2)\n",
        "        # set the gain appropriately for our activation function \n",
        "        nn.init.xavier_uniform_(self.fc_self.weight, gain=gain)\n",
        "        nn.init.xavier_uniform_(self.fc_neigh.weight, gain=gain)\n",
        "    \n",
        "    def forward(self, graph, features):\n",
        "        \"\"\"\n",
        "        The following code is DGL's way of using the graph class\n",
        "        to facilitate message passing. The equivalent code in pure pytorch\n",
        "        operating instead on the adjacency matrix adj and the feature matrix x would be:\n",
        "            \n",
        "            def forward(self, x, adj):\n",
        "                return adj.matmul(x, reduce='mean') @ self_weights + x @ neigh_weights + bias\n",
        "        \n",
        "        \"\"\"\n",
        "        # set the incoming features matrix as the input to this layer 'h'\n",
        "        graph.srcdata['h'] = features\n",
        "        # create 2 user defined functions, the first to collect features \n",
        "        # from the src nodes 'h', send along edges 'm', and aggregate them at the \n",
        "        # destination nodes (the neighbours)\n",
        "        features_from_src_nodes = dgl.function.copy_src('h', 'm')\n",
        "        aggregation_at_dst_nodes = dgl.function.mean('m', 'neigh')\n",
        "        # graph.update_all is a helper function to send the first function\n",
        "        # along the edges and recieve the second function at the\n",
        "        # destination nodes\n",
        "        graph.update_all(features_from_src_nodes, aggregation_at_dst_nodes)\n",
        "        # now we can get our aggregated neighbourhood features\n",
        "        h_neigh = graph.dstdata['neigh']\n",
        "        # and combine them with the src features (self loops)\n",
        "        # fc_self(features) is equivalent to features @ weights + biases\n",
        "        output = self.fc_self(features) + self.fc_neigh(h_neigh)\n",
        "        # lastly we add a nonlinearity to the output enabling backpropogation\n",
        "        output = F.relu(output)\n",
        "        return output\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXaVbcWDt2wg"
      },
      "source": [
        "The only method we need will be the 'self.forward' method (forward pass). The backpropogation will be handled by the library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0X6r4mnt2wg"
      },
      "source": [
        "Now lets build a graphSAGE GNN out of these layers that takes in a DGLGraph we made previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHdPyYIt2wh"
      },
      "source": [
        "class SimpleGraphSAGE(nn.Module):\n",
        "    def __init__(\n",
        "            self, \n",
        "            g, \n",
        "            n_features, \n",
        "            n_hidden, \n",
        "            n_classes, \n",
        "            n_layers\n",
        "    ):\n",
        "        super(SimpleGraphSAGE, self).__init__()\n",
        "        # A ModuleList will hold all of our layers\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.g = g\n",
        "\n",
        "        # input layer, the input size of which will be \n",
        "        # the number of features\n",
        "        self.conv_layers.append(MeanAggSageLayer(n_features, n_hidden))\n",
        "        # create the hidden layers: (n_layers - 1) allowing for the output layer\n",
        "        for i in range(n_layers - 1):\n",
        "            self.conv_layers.append(MeanAggSageLayer(n_hidden, n_hidden))\n",
        "        # output layer, the output size of which will be the number of classes\n",
        "        self.conv_layers.append(MeanAggSageLayer(n_hidden, n_classes))\n",
        "\n",
        "    def forward(self, features):\n",
        "        # h(0) will be equal to the feature matrix\n",
        "        h = features\n",
        "        for conv in self.conv_layers:\n",
        "            # pass h through one layer and back into the next\n",
        "            h = conv(self.g, h)\n",
        "        # now we have h(k)\n",
        "        return h"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDGdK2-jt2wh"
      },
      "source": [
        "Before we create one of these models we need to decide on some params:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfBImZP1t2wh"
      },
      "source": [
        "n_hidden = 16\n",
        "n_layers = 2\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.0005\n",
        "n_epochs = 120"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOsLIP2at2wh"
      },
      "source": [
        "Now we can create a GraphSAGE model using our graph (g)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rdhSHw9t2wi"
      },
      "source": [
        "model = SimpleGraphSAGE(g, n_features, n_hidden, n_classes, n_layers)\n",
        "# we can send this to gpu memory as well\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF6xpKktt2wi"
      },
      "source": [
        "# use cross entropy loss function\n",
        "loss_fcn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# use Adam Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=weight_decay)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j13azUCOt2wj"
      },
      "source": [
        "# we also need a scoring function, lets create a simple accuracy calculator:\n",
        "def get_accuracy(pred, true):\n",
        "    _, indices = torch.max(pred, dim=1)\n",
        "    correct = torch.sum(indices == true)\n",
        "    return correct.item() * 1.0 / len(true)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSr3wugRt2wj"
      },
      "source": [
        "And we can decide on a simple training routine too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uzrtvv-t2wk"
      },
      "source": [
        "# now our training pipeline is able to be built\n",
        "def train(model, optimizer, n_epochs):\n",
        "    # we will keep track how long each epoch takes so we can calculate things like\n",
        "    # Traversed Edges Per Second (TEPS)\n",
        "    dur = []\n",
        "    all_train_logits = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # This doesnt train the mdoel, instead it tells all the child modules\n",
        "        # that the model is in training mode and not evaluating mode\n",
        "        # (for examplee, when evaluating, you dont want to apply dropout to the input tensor)\n",
        "        model.train()\n",
        "        t0 = time.time()\n",
        "\n",
        "        # the forward pass - sending the features to the model.forward method\n",
        "        output = model(features)\n",
        "        # calculate our current loss by comparing only the training nodes'\n",
        "        # prediction and truth\n",
        "        output_train = output[train_mask]\n",
        "        loss = loss_fcn(output_train, labels[train_mask])\n",
        "\n",
        "        # the backwards pass! update the weights in our SAGELayers - but first:\n",
        "        # reset the gradient back to 0 before doing backpropogation\n",
        "        # (pytorch by default accumulates the gradients after each backward pass)\n",
        "        optimizer.zero_grad()\n",
        "        # backpropogation\n",
        "        loss.backward()\n",
        "        # step the adam optimizer forward\n",
        "        optimizer.step()\n",
        "\n",
        "        dur.append(time.time() - t0)\n",
        "\n",
        "        # set the model into evaluation model\n",
        "        model.eval()\n",
        "        # temporatily turn off the gradient calculation as\n",
        "        # just want to simply inference\n",
        "        with torch.no_grad():\n",
        "            output_val = output[val_mask]\n",
        "            labels_val = labels[val_mask]\n",
        "            acc = get_accuracy(output_val, labels_val)\n",
        "\n",
        "        # record the output logits for plotting later\n",
        "        all_train_logits.append(output_train)\n",
        "\n",
        "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
        "              \"TEPS {:.2f}\".format(epoch, np.mean(dur), loss.item(), acc, \n",
        "                                   n_edges / np.mean(dur)))\n",
        "\n",
        "    print('training complete')\n",
        "    return model, output, all_train_logits\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9qTstv_t2wk",
        "outputId": "4d7dfd81-5f85-4074-a4f4-7855b1f2df70"
      },
      "source": [
        "model, last_output, _ = train(model, optimizer, n_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Time(s) 1.6646 | Loss 2.0308 | Accuracy 0.1220 | TEPS 6341.54\n",
            "Epoch 00001 | Time(s) 0.8352 | Loss 1.9779 | Accuracy 0.1480 | TEPS 12638.96\n",
            "Epoch 00002 | Time(s) 0.5585 | Loss 1.9483 | Accuracy 0.2620 | TEPS 18899.54\n",
            "Epoch 00003 | Time(s) 0.4202 | Loss 1.9185 | Accuracy 0.2020 | TEPS 25121.57\n",
            "Epoch 00004 | Time(s) 0.3372 | Loss 1.8914 | Accuracy 0.2600 | TEPS 31305.99\n",
            "Epoch 00005 | Time(s) 0.2818 | Loss 1.8588 | Accuracy 0.3700 | TEPS 37452.66\n",
            "Epoch 00006 | Time(s) 0.2423 | Loss 1.8245 | Accuracy 0.4020 | TEPS 43562.81\n",
            "Epoch 00007 | Time(s) 0.2128 | Loss 1.7815 | Accuracy 0.4120 | TEPS 49595.58\n",
            "Epoch 00008 | Time(s) 0.1898 | Loss 1.7272 | Accuracy 0.4300 | TEPS 55607.50\n",
            "Epoch 00009 | Time(s) 0.1714 | Loss 1.6670 | Accuracy 0.4180 | TEPS 61596.90\n",
            "Epoch 00010 | Time(s) 0.1563 | Loss 1.6031 | Accuracy 0.4240 | TEPS 67546.34\n",
            "Epoch 00011 | Time(s) 0.1437 | Loss 1.5383 | Accuracy 0.4280 | TEPS 73466.26\n",
            "Epoch 00012 | Time(s) 0.1330 | Loss 1.4640 | Accuracy 0.4920 | TEPS 79351.13\n",
            "Epoch 00013 | Time(s) 0.1239 | Loss 1.3895 | Accuracy 0.5000 | TEPS 85201.02\n",
            "Epoch 00014 | Time(s) 0.1160 | Loss 1.3132 | Accuracy 0.4980 | TEPS 91014.80\n",
            "Epoch 00015 | Time(s) 0.1091 | Loss 1.2368 | Accuracy 0.4880 | TEPS 96797.51\n",
            "Epoch 00016 | Time(s) 0.1030 | Loss 1.1608 | Accuracy 0.4900 | TEPS 102527.88\n",
            "Epoch 00017 | Time(s) 0.0975 | Loss 1.0865 | Accuracy 0.5100 | TEPS 108240.68\n",
            "Epoch 00018 | Time(s) 0.0927 | Loss 1.0160 | Accuracy 0.5200 | TEPS 113922.23\n",
            "Epoch 00019 | Time(s) 0.0883 | Loss 0.9489 | Accuracy 0.5300 | TEPS 119572.00\n",
            "Epoch 00020 | Time(s) 0.0843 | Loss 0.8856 | Accuracy 0.5420 | TEPS 125176.93\n",
            "Epoch 00021 | Time(s) 0.0807 | Loss 0.8268 | Accuracy 0.5520 | TEPS 130733.72\n",
            "Epoch 00022 | Time(s) 0.0775 | Loss 0.7707 | Accuracy 0.5580 | TEPS 136261.36\n",
            "Epoch 00023 | Time(s) 0.0745 | Loss 0.7181 | Accuracy 0.5680 | TEPS 141655.17\n",
            "Epoch 00024 | Time(s) 0.0718 | Loss 0.6684 | Accuracy 0.5880 | TEPS 147036.15\n",
            "Epoch 00025 | Time(s) 0.0692 | Loss 0.6216 | Accuracy 0.6040 | TEPS 152471.76\n",
            "Epoch 00026 | Time(s) 0.0669 | Loss 0.5783 | Accuracy 0.6180 | TEPS 157874.62\n",
            "Epoch 00027 | Time(s) 0.0647 | Loss 0.5380 | Accuracy 0.6180 | TEPS 163244.28\n",
            "Epoch 00028 | Time(s) 0.0626 | Loss 0.5012 | Accuracy 0.6340 | TEPS 168584.63\n",
            "Epoch 00029 | Time(s) 0.0607 | Loss 0.4683 | Accuracy 0.6400 | TEPS 173900.41\n",
            "Epoch 00030 | Time(s) 0.0589 | Loss 0.4392 | Accuracy 0.6460 | TEPS 179175.61\n",
            "Epoch 00031 | Time(s) 0.0572 | Loss 0.4140 | Accuracy 0.6520 | TEPS 184407.82\n",
            "Epoch 00032 | Time(s) 0.0557 | Loss 0.3925 | Accuracy 0.6480 | TEPS 189616.01\n",
            "Epoch 00033 | Time(s) 0.0542 | Loss 0.3748 | Accuracy 0.6380 | TEPS 194808.57\n",
            "Epoch 00034 | Time(s) 0.0528 | Loss 0.3606 | Accuracy 0.6340 | TEPS 199956.28\n",
            "Epoch 00035 | Time(s) 0.0516 | Loss 0.3493 | Accuracy 0.6320 | TEPS 204649.29\n",
            "Epoch 00036 | Time(s) 0.0503 | Loss 0.3402 | Accuracy 0.6360 | TEPS 209691.42\n",
            "Epoch 00037 | Time(s) 0.0492 | Loss 0.3325 | Accuracy 0.6360 | TEPS 214748.48\n",
            "Epoch 00038 | Time(s) 0.0480 | Loss 0.3263 | Accuracy 0.6400 | TEPS 219758.59\n",
            "Epoch 00039 | Time(s) 0.0470 | Loss 0.3210 | Accuracy 0.6440 | TEPS 224742.74\n",
            "Epoch 00040 | Time(s) 0.0460 | Loss 0.3165 | Accuracy 0.6500 | TEPS 229700.84\n",
            "Epoch 00041 | Time(s) 0.0450 | Loss 0.3128 | Accuracy 0.6560 | TEPS 234619.73\n",
            "Epoch 00042 | Time(s) 0.0441 | Loss 0.3094 | Accuracy 0.6600 | TEPS 239495.12\n",
            "Epoch 00043 | Time(s) 0.0432 | Loss 0.3064 | Accuracy 0.6560 | TEPS 244373.62\n",
            "Epoch 00044 | Time(s) 0.0424 | Loss 0.3040 | Accuracy 0.6540 | TEPS 249212.11\n",
            "Epoch 00045 | Time(s) 0.0416 | Loss 0.3022 | Accuracy 0.6480 | TEPS 254039.14\n",
            "Epoch 00046 | Time(s) 0.0408 | Loss 0.3012 | Accuracy 0.6480 | TEPS 258847.67\n",
            "Epoch 00047 | Time(s) 0.0400 | Loss 0.2996 | Accuracy 0.6480 | TEPS 263628.08\n",
            "Epoch 00048 | Time(s) 0.0393 | Loss 0.2991 | Accuracy 0.6520 | TEPS 268358.50\n",
            "Epoch 00049 | Time(s) 0.0387 | Loss 0.2981 | Accuracy 0.6480 | TEPS 273074.68\n",
            "Epoch 00050 | Time(s) 0.0381 | Loss 0.2982 | Accuracy 0.6540 | TEPS 277391.68\n",
            "Epoch 00051 | Time(s) 0.0375 | Loss 0.2971 | Accuracy 0.6520 | TEPS 281610.12\n",
            "Epoch 00052 | Time(s) 0.0369 | Loss 0.2974 | Accuracy 0.6540 | TEPS 286244.57\n",
            "Epoch 00053 | Time(s) 0.0364 | Loss 0.2969 | Accuracy 0.6520 | TEPS 289872.44\n",
            "Epoch 00054 | Time(s) 0.0359 | Loss 0.2971 | Accuracy 0.6520 | TEPS 294444.53\n",
            "Epoch 00055 | Time(s) 0.0353 | Loss 0.2972 | Accuracy 0.6540 | TEPS 299002.65\n",
            "Epoch 00056 | Time(s) 0.0348 | Loss 0.2975 | Accuracy 0.6500 | TEPS 303503.31\n",
            "Epoch 00057 | Time(s) 0.0343 | Loss 0.2973 | Accuracy 0.6500 | TEPS 307969.30\n",
            "Epoch 00058 | Time(s) 0.0338 | Loss 0.2974 | Accuracy 0.6500 | TEPS 312380.37\n",
            "Epoch 00059 | Time(s) 0.0333 | Loss 0.2975 | Accuracy 0.6500 | TEPS 316760.66\n",
            "Epoch 00060 | Time(s) 0.0329 | Loss 0.2977 | Accuracy 0.6500 | TEPS 321110.72\n",
            "Epoch 00061 | Time(s) 0.0325 | Loss 0.2983 | Accuracy 0.6520 | TEPS 325042.63\n",
            "Epoch 00062 | Time(s) 0.0320 | Loss 0.2984 | Accuracy 0.6520 | TEPS 329380.60\n",
            "Epoch 00063 | Time(s) 0.0316 | Loss 0.2987 | Accuracy 0.6520 | TEPS 333733.15\n",
            "Epoch 00064 | Time(s) 0.0312 | Loss 0.2993 | Accuracy 0.6520 | TEPS 338071.24\n",
            "Epoch 00065 | Time(s) 0.0308 | Loss 0.3016 | Accuracy 0.6520 | TEPS 342383.26\n",
            "Epoch 00066 | Time(s) 0.0305 | Loss 0.3049 | Accuracy 0.6600 | TEPS 346648.25\n",
            "Epoch 00067 | Time(s) 0.0301 | Loss 0.2998 | Accuracy 0.6540 | TEPS 350378.94\n",
            "Epoch 00068 | Time(s) 0.0298 | Loss 0.3030 | Accuracy 0.6480 | TEPS 354377.30\n",
            "Epoch 00069 | Time(s) 0.0294 | Loss 0.3010 | Accuracy 0.6520 | TEPS 358585.08\n",
            "Epoch 00070 | Time(s) 0.0291 | Loss 0.3023 | Accuracy 0.6560 | TEPS 362759.38\n",
            "Epoch 00071 | Time(s) 0.0288 | Loss 0.3040 | Accuracy 0.6560 | TEPS 366957.07\n",
            "Epoch 00072 | Time(s) 0.0284 | Loss 0.3023 | Accuracy 0.6420 | TEPS 371143.35\n",
            "Epoch 00073 | Time(s) 0.0281 | Loss 0.3023 | Accuracy 0.6420 | TEPS 375261.65\n",
            "Epoch 00074 | Time(s) 0.0278 | Loss 0.3015 | Accuracy 0.6540 | TEPS 379391.14\n",
            "Epoch 00075 | Time(s) 0.0275 | Loss 0.3012 | Accuracy 0.6520 | TEPS 383485.87\n",
            "Epoch 00076 | Time(s) 0.0272 | Loss 0.3004 | Accuracy 0.6540 | TEPS 387525.70\n",
            "Epoch 00077 | Time(s) 0.0270 | Loss 0.3010 | Accuracy 0.6560 | TEPS 391581.47\n",
            "Epoch 00078 | Time(s) 0.0267 | Loss 0.2986 | Accuracy 0.6500 | TEPS 395638.86\n",
            "Epoch 00079 | Time(s) 0.0264 | Loss 0.2991 | Accuracy 0.6480 | TEPS 399685.70\n",
            "Epoch 00080 | Time(s) 0.0262 | Loss 0.2985 | Accuracy 0.6560 | TEPS 403563.11\n",
            "Epoch 00081 | Time(s) 0.0259 | Loss 0.2976 | Accuracy 0.6520 | TEPS 407210.13\n",
            "Epoch 00082 | Time(s) 0.0257 | Loss 0.2967 | Accuracy 0.6560 | TEPS 411056.12\n",
            "Epoch 00083 | Time(s) 0.0254 | Loss 0.2969 | Accuracy 0.6560 | TEPS 415012.67\n",
            "Epoch 00084 | Time(s) 0.0252 | Loss 0.2960 | Accuracy 0.6560 | TEPS 418939.54\n",
            "Epoch 00085 | Time(s) 0.0250 | Loss 0.2957 | Accuracy 0.6580 | TEPS 422860.83\n",
            "Epoch 00086 | Time(s) 0.0247 | Loss 0.2952 | Accuracy 0.6540 | TEPS 426770.72\n",
            "Epoch 00087 | Time(s) 0.0245 | Loss 0.2956 | Accuracy 0.6540 | TEPS 430653.91\n",
            "Epoch 00088 | Time(s) 0.0243 | Loss 0.2950 | Accuracy 0.6520 | TEPS 434515.20\n",
            "Epoch 00089 | Time(s) 0.0241 | Loss 0.2946 | Accuracy 0.6520 | TEPS 438377.81\n",
            "Epoch 00090 | Time(s) 0.0239 | Loss 0.2947 | Accuracy 0.6560 | TEPS 442213.37\n",
            "Epoch 00091 | Time(s) 0.0237 | Loss 0.2949 | Accuracy 0.6560 | TEPS 446016.59\n",
            "Epoch 00092 | Time(s) 0.0235 | Loss 0.2946 | Accuracy 0.6580 | TEPS 449788.52\n",
            "Epoch 00093 | Time(s) 0.0233 | Loss 0.2950 | Accuracy 0.6560 | TEPS 453574.56\n",
            "Epoch 00094 | Time(s) 0.0231 | Loss 0.2953 | Accuracy 0.6580 | TEPS 457362.05\n",
            "Epoch 00095 | Time(s) 0.0229 | Loss 0.2981 | Accuracy 0.6620 | TEPS 461100.81\n",
            "Epoch 00096 | Time(s) 0.0227 | Loss 0.2950 | Accuracy 0.6580 | TEPS 464843.50\n",
            "Epoch 00097 | Time(s) 0.0225 | Loss 0.2970 | Accuracy 0.6520 | TEPS 468570.34\n",
            "Epoch 00098 | Time(s) 0.0224 | Loss 0.2946 | Accuracy 0.6540 | TEPS 472286.32\n",
            "Epoch 00099 | Time(s) 0.0222 | Loss 0.2958 | Accuracy 0.6600 | TEPS 475880.38\n",
            "Epoch 00100 | Time(s) 0.0220 | Loss 0.2961 | Accuracy 0.6600 | TEPS 479542.57\n",
            "Epoch 00101 | Time(s) 0.0218 | Loss 0.2955 | Accuracy 0.6560 | TEPS 483205.91\n",
            "Epoch 00102 | Time(s) 0.0217 | Loss 0.2957 | Accuracy 0.6480 | TEPS 486597.56\n",
            "Epoch 00103 | Time(s) 0.0215 | Loss 0.2954 | Accuracy 0.6440 | TEPS 489988.08\n",
            "Epoch 00104 | Time(s) 0.0214 | Loss 0.2953 | Accuracy 0.6500 | TEPS 493305.79\n",
            "Epoch 00105 | Time(s) 0.0213 | Loss 0.2942 | Accuracy 0.6580 | TEPS 496537.18\n",
            "Epoch 00106 | Time(s) 0.0211 | Loss 0.2942 | Accuracy 0.6620 | TEPS 500093.98\n",
            "Epoch 00107 | Time(s) 0.0210 | Loss 0.2942 | Accuracy 0.6620 | TEPS 503662.93\n",
            "Epoch 00108 | Time(s) 0.0208 | Loss 0.2939 | Accuracy 0.6600 | TEPS 507197.53\n",
            "Epoch 00109 | Time(s) 0.0207 | Loss 0.2933 | Accuracy 0.6620 | TEPS 510679.72\n",
            "Epoch 00110 | Time(s) 0.0205 | Loss 0.2927 | Accuracy 0.6620 | TEPS 514173.94\n",
            "Epoch 00111 | Time(s) 0.0204 | Loss 0.2926 | Accuracy 0.6600 | TEPS 517667.58\n",
            "Epoch 00112 | Time(s) 0.0203 | Loss 0.2924 | Accuracy 0.6620 | TEPS 521114.77\n",
            "Epoch 00113 | Time(s) 0.0201 | Loss 0.2924 | Accuracy 0.6620 | TEPS 524574.12\n",
            "Epoch 00114 | Time(s) 0.0200 | Loss 0.2941 | Accuracy 0.6600 | TEPS 528022.34\n",
            "Epoch 00115 | Time(s) 0.0199 | Loss 0.2947 | Accuracy 0.6580 | TEPS 531353.02\n",
            "Epoch 00116 | Time(s) 0.0197 | Loss 0.2927 | Accuracy 0.6600 | TEPS 534693.58\n",
            "Epoch 00117 | Time(s) 0.0196 | Loss 0.2920 | Accuracy 0.6640 | TEPS 538075.23\n",
            "Epoch 00118 | Time(s) 0.0195 | Loss 0.2919 | Accuracy 0.6600 | TEPS 541459.75\n",
            "Epoch 00119 | Time(s) 0.0194 | Loss 0.2919 | Accuracy 0.6580 | TEPS 544830.49\n",
            "training complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDebhJlnt2wk",
        "outputId": "02ef9b74-4f94-448f-93bb-5b218e5f0019"
      },
      "source": [
        "# now we can evaluate the model on the test set\n",
        "output_test = last_output[test_mask]\n",
        "labels_test = labels[test_mask]\n",
        "acc = get_accuracy(output_test, labels_test)\n",
        "print(\"Test Accuracy {:.4f}\".format(acc))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.6790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-SnGxmt2wl"
      },
      "source": [
        "## Further experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46pEiaQ8t2wl"
      },
      "source": [
        "Ok so not too impressive - how can we improve the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFOh5VNvt2wl"
      },
      "source": [
        "For one, there are other aggregation methods used in the original paper.\n",
        "DGL has implemented a SAGEConv layer that takes our simplified SageLayer further:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H1hH-Ikt2wm"
      },
      "source": [
        "from dgl.nn.pytorch.conv.sageconv import SAGEConv"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_nW2rott2wm"
      },
      "source": [
        "# A new graphSAGE net could be built as follows:\n",
        "class GraphSAGE(nn.Module):\n",
        "    \"\"\"\n",
        "    GraphSAGE pytorch implementation from paper `Inductive Representation Learning on\n",
        "    Large Graphs <https://arxiv.org/pdf/1706.02216.pdf>`__.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            g,\n",
        "            n_features,\n",
        "            n_hidden,\n",
        "            n_classes,\n",
        "            n_layers,\n",
        "            agg,\n",
        "            activation,\n",
        "            dropout,\n",
        "    ):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.g = g\n",
        "\n",
        "        # input layer\n",
        "        self.layers.append(\n",
        "            SAGEConv(n_features, n_hidden, agg, feat_drop=dropout, activation=activation)\n",
        "        )\n",
        "        # hidden layers\n",
        "        for i in range(n_layers - 1):\n",
        "            self.layers.append(\n",
        "                SAGEConv(n_hidden, n_hidden, agg, feat_drop=dropout, activation=activation)\n",
        "            )\n",
        "        # output layer\n",
        "        self.layers.append(\n",
        "            SAGEConv(n_hidden, n_classes, agg, feat_drop=dropout, activation=None)\n",
        "        ) # no activation None for final layer\n",
        "\n",
        "    def forward(self, features):\n",
        "        h = features\n",
        "        for layer in self.layers:\n",
        "            h = layer(self.g, h)\n",
        "        return h"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itDo74MHt2wm"
      },
      "source": [
        "The 'agg' variable can now be one of ['mean', 'gcn', 'pool', 'lstm'].\n",
        "Additionally, a dropout fraction can be set, activation can be changed from 'relu', and the SAGEConv layer also supports an optional normalization function.\n",
        "\n",
        "We'll start by looking at the **mean** aggregation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k63Ao5E3t2wm"
      },
      "source": [
        "# lets try our same params as before but using a dropout value of 0.5\n",
        "model = GraphSAGE(g, n_features, n_hidden, n_classes, n_layers, 'mean', F.relu, 0.5)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "# initialize the optimzier again as the model params have changed\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwN6wPbWt2wn",
        "outputId": "9c481b56-8fd8-411d-a7e3-f8ca2b3aa93d"
      },
      "source": [
        "model, last_output, all_train_logits = train(model, optimizer, n_epochs)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Time(s) 0.0095 | Loss 1.9609 | Accuracy 0.1740 | TEPS 1106296.02\n",
            "Epoch 00001 | Time(s) 0.0075 | Loss 1.9309 | Accuracy 0.2180 | TEPS 1401154.25\n",
            "Epoch 00002 | Time(s) 0.0067 | Loss 1.9287 | Accuracy 0.2360 | TEPS 1577271.87\n",
            "Epoch 00003 | Time(s) 0.0063 | Loss 1.9125 | Accuracy 0.2100 | TEPS 1678389.39\n",
            "Epoch 00004 | Time(s) 0.0061 | Loss 1.8853 | Accuracy 0.1860 | TEPS 1738145.03\n",
            "Epoch 00005 | Time(s) 0.0059 | Loss 1.8630 | Accuracy 0.2400 | TEPS 1790435.11\n",
            "Epoch 00006 | Time(s) 0.0061 | Loss 1.8478 | Accuracy 0.2520 | TEPS 1716791.55\n",
            "Epoch 00007 | Time(s) 0.0060 | Loss 1.8173 | Accuracy 0.2660 | TEPS 1746387.59\n",
            "Epoch 00008 | Time(s) 0.0060 | Loss 1.7918 | Accuracy 0.2660 | TEPS 1773351.63\n",
            "Epoch 00009 | Time(s) 0.0059 | Loss 1.7333 | Accuracy 0.3340 | TEPS 1799902.15\n",
            "Epoch 00010 | Time(s) 0.0058 | Loss 1.6893 | Accuracy 0.2820 | TEPS 1823861.75\n",
            "Epoch 00011 | Time(s) 0.0057 | Loss 1.6241 | Accuracy 0.3180 | TEPS 1845775.28\n",
            "Epoch 00012 | Time(s) 0.0059 | Loss 1.6680 | Accuracy 0.2680 | TEPS 1802149.61\n",
            "Epoch 00013 | Time(s) 0.0058 | Loss 1.5094 | Accuracy 0.3200 | TEPS 1817654.32\n",
            "Epoch 00014 | Time(s) 0.0058 | Loss 1.5323 | Accuracy 0.3300 | TEPS 1835804.37\n",
            "Epoch 00015 | Time(s) 0.0057 | Loss 1.4905 | Accuracy 0.3560 | TEPS 1851419.32\n",
            "Epoch 00016 | Time(s) 0.0056 | Loss 1.4949 | Accuracy 0.3320 | TEPS 1873279.58\n",
            "Epoch 00017 | Time(s) 0.0056 | Loss 1.3726 | Accuracy 0.4000 | TEPS 1886503.02\n",
            "Epoch 00018 | Time(s) 0.0056 | Loss 1.3557 | Accuracy 0.4200 | TEPS 1896790.05\n",
            "Epoch 00019 | Time(s) 0.0056 | Loss 1.2566 | Accuracy 0.3700 | TEPS 1878119.59\n",
            "Epoch 00020 | Time(s) 0.0056 | Loss 1.1826 | Accuracy 0.4120 | TEPS 1889286.66\n",
            "Epoch 00021 | Time(s) 0.0056 | Loss 1.1732 | Accuracy 0.4120 | TEPS 1899065.54\n",
            "Epoch 00022 | Time(s) 0.0055 | Loss 1.1869 | Accuracy 0.4360 | TEPS 1908415.47\n",
            "Epoch 00023 | Time(s) 0.0055 | Loss 1.1886 | Accuracy 0.4600 | TEPS 1917375.36\n",
            "Epoch 00024 | Time(s) 0.0055 | Loss 1.1313 | Accuracy 0.4720 | TEPS 1904432.00\n",
            "Epoch 00025 | Time(s) 0.0056 | Loss 1.0481 | Accuracy 0.5220 | TEPS 1894697.52\n",
            "Epoch 00026 | Time(s) 0.0056 | Loss 0.9786 | Accuracy 0.4680 | TEPS 1873081.71\n",
            "Epoch 00027 | Time(s) 0.0056 | Loss 0.9252 | Accuracy 0.4960 | TEPS 1880975.50\n",
            "Epoch 00028 | Time(s) 0.0056 | Loss 0.9245 | Accuracy 0.4940 | TEPS 1888584.92\n",
            "Epoch 00029 | Time(s) 0.0056 | Loss 0.8641 | Accuracy 0.5420 | TEPS 1895753.62\n",
            "Epoch 00030 | Time(s) 0.0056 | Loss 0.8479 | Accuracy 0.5460 | TEPS 1899426.19\n",
            "Epoch 00031 | Time(s) 0.0055 | Loss 0.7645 | Accuracy 0.5420 | TEPS 1905892.46\n",
            "Epoch 00032 | Time(s) 0.0055 | Loss 0.8607 | Accuracy 0.5120 | TEPS 1912049.61\n",
            "Epoch 00033 | Time(s) 0.0055 | Loss 0.7246 | Accuracy 0.5300 | TEPS 1914665.94\n",
            "Epoch 00034 | Time(s) 0.0055 | Loss 0.7236 | Accuracy 0.5600 | TEPS 1910497.63\n",
            "Epoch 00035 | Time(s) 0.0055 | Loss 0.6816 | Accuracy 0.5260 | TEPS 1903974.38\n",
            "Epoch 00036 | Time(s) 0.0055 | Loss 0.6621 | Accuracy 0.5880 | TEPS 1907841.57\n",
            "Epoch 00037 | Time(s) 0.0055 | Loss 0.5739 | Accuracy 0.5840 | TEPS 1913091.23\n",
            "Epoch 00038 | Time(s) 0.0055 | Loss 0.5775 | Accuracy 0.5820 | TEPS 1918077.14\n",
            "Epoch 00039 | Time(s) 0.0055 | Loss 0.5095 | Accuracy 0.5460 | TEPS 1922748.13\n",
            "Epoch 00040 | Time(s) 0.0055 | Loss 0.5363 | Accuracy 0.5460 | TEPS 1927611.48\n",
            "Epoch 00041 | Time(s) 0.0055 | Loss 0.5093 | Accuracy 0.5920 | TEPS 1914329.70\n",
            "Epoch 00042 | Time(s) 0.0055 | Loss 0.5623 | Accuracy 0.5960 | TEPS 1915524.33\n",
            "Epoch 00043 | Time(s) 0.0055 | Loss 0.4503 | Accuracy 0.5740 | TEPS 1920232.95\n",
            "Epoch 00044 | Time(s) 0.0055 | Loss 0.4479 | Accuracy 0.5700 | TEPS 1924529.02\n",
            "Epoch 00045 | Time(s) 0.0055 | Loss 0.4873 | Accuracy 0.5960 | TEPS 1928373.27\n",
            "Epoch 00046 | Time(s) 0.0055 | Loss 0.3741 | Accuracy 0.6420 | TEPS 1932411.05\n",
            "Epoch 00047 | Time(s) 0.0055 | Loss 0.3735 | Accuracy 0.6220 | TEPS 1936264.73\n",
            "Epoch 00048 | Time(s) 0.0054 | Loss 0.4377 | Accuracy 0.5740 | TEPS 1939847.22\n",
            "Epoch 00049 | Time(s) 0.0054 | Loss 0.4148 | Accuracy 0.5980 | TEPS 1937349.99\n",
            "Epoch 00050 | Time(s) 0.0054 | Loss 0.3778 | Accuracy 0.6240 | TEPS 1940864.32\n",
            "Epoch 00051 | Time(s) 0.0054 | Loss 0.3754 | Accuracy 0.5860 | TEPS 1942819.94\n",
            "Epoch 00052 | Time(s) 0.0054 | Loss 0.3487 | Accuracy 0.6220 | TEPS 1946341.08\n",
            "Epoch 00053 | Time(s) 0.0054 | Loss 0.3532 | Accuracy 0.6240 | TEPS 1948844.35\n",
            "Epoch 00054 | Time(s) 0.0054 | Loss 0.3524 | Accuracy 0.5940 | TEPS 1952174.67\n",
            "Epoch 00055 | Time(s) 0.0054 | Loss 0.2925 | Accuracy 0.5900 | TEPS 1954995.97\n",
            "Epoch 00056 | Time(s) 0.0054 | Loss 0.3605 | Accuracy 0.6160 | TEPS 1957583.27\n",
            "Epoch 00057 | Time(s) 0.0054 | Loss 0.2812 | Accuracy 0.6520 | TEPS 1958643.69\n",
            "Epoch 00058 | Time(s) 0.0054 | Loss 0.2502 | Accuracy 0.6600 | TEPS 1961146.37\n",
            "Epoch 00059 | Time(s) 0.0054 | Loss 0.2576 | Accuracy 0.6480 | TEPS 1963551.39\n",
            "Epoch 00060 | Time(s) 0.0054 | Loss 0.1870 | Accuracy 0.6280 | TEPS 1948047.90\n",
            "Epoch 00061 | Time(s) 0.0054 | Loss 0.2669 | Accuracy 0.6580 | TEPS 1949213.89\n",
            "Epoch 00062 | Time(s) 0.0054 | Loss 0.2639 | Accuracy 0.6780 | TEPS 1950655.17\n",
            "Epoch 00063 | Time(s) 0.0054 | Loss 0.2240 | Accuracy 0.6240 | TEPS 1950709.60\n",
            "Epoch 00064 | Time(s) 0.0054 | Loss 0.2204 | Accuracy 0.6360 | TEPS 1950946.19\n",
            "Epoch 00065 | Time(s) 0.0054 | Loss 0.1793 | Accuracy 0.6260 | TEPS 1952273.27\n",
            "Epoch 00066 | Time(s) 0.0054 | Loss 0.2350 | Accuracy 0.6460 | TEPS 1953795.35\n",
            "Epoch 00067 | Time(s) 0.0054 | Loss 0.1955 | Accuracy 0.6520 | TEPS 1955653.42\n",
            "Epoch 00068 | Time(s) 0.0054 | Loss 0.2891 | Accuracy 0.6480 | TEPS 1940627.24\n",
            "Epoch 00069 | Time(s) 0.0055 | Loss 0.1971 | Accuracy 0.6440 | TEPS 1928061.73\n",
            "Epoch 00070 | Time(s) 0.0055 | Loss 0.2492 | Accuracy 0.6680 | TEPS 1916215.34\n",
            "Epoch 00071 | Time(s) 0.0055 | Loss 0.1789 | Accuracy 0.6400 | TEPS 1910859.81\n",
            "Epoch 00072 | Time(s) 0.0055 | Loss 0.2277 | Accuracy 0.6600 | TEPS 1911894.34\n",
            "Epoch 00073 | Time(s) 0.0055 | Loss 0.2303 | Accuracy 0.6580 | TEPS 1912811.53\n",
            "Epoch 00074 | Time(s) 0.0055 | Loss 0.1935 | Accuracy 0.6480 | TEPS 1913548.51\n",
            "Epoch 00075 | Time(s) 0.0056 | Loss 0.1750 | Accuracy 0.6500 | TEPS 1900135.10\n",
            "Epoch 00076 | Time(s) 0.0056 | Loss 0.2282 | Accuracy 0.6480 | TEPS 1894574.45\n",
            "Epoch 00077 | Time(s) 0.0056 | Loss 0.2573 | Accuracy 0.6580 | TEPS 1895089.49\n",
            "Epoch 00078 | Time(s) 0.0056 | Loss 0.2088 | Accuracy 0.6520 | TEPS 1896589.82\n",
            "Epoch 00079 | Time(s) 0.0056 | Loss 0.1594 | Accuracy 0.6280 | TEPS 1897129.80\n",
            "Epoch 00080 | Time(s) 0.0056 | Loss 0.2467 | Accuracy 0.6520 | TEPS 1898317.70\n",
            "Epoch 00081 | Time(s) 0.0056 | Loss 0.1917 | Accuracy 0.6820 | TEPS 1899317.08\n",
            "Epoch 00082 | Time(s) 0.0056 | Loss 0.2840 | Accuracy 0.6620 | TEPS 1898816.61\n",
            "Epoch 00083 | Time(s) 0.0056 | Loss 0.2527 | Accuracy 0.6400 | TEPS 1899763.46\n",
            "Epoch 00084 | Time(s) 0.0056 | Loss 0.1486 | Accuracy 0.6580 | TEPS 1899826.35\n",
            "Epoch 00085 | Time(s) 0.0056 | Loss 0.1189 | Accuracy 0.6720 | TEPS 1897898.16\n",
            "Epoch 00086 | Time(s) 0.0056 | Loss 0.1523 | Accuracy 0.6780 | TEPS 1896855.58\n",
            "Epoch 00087 | Time(s) 0.0056 | Loss 0.1548 | Accuracy 0.6680 | TEPS 1897654.08\n",
            "Epoch 00088 | Time(s) 0.0056 | Loss 0.1376 | Accuracy 0.6500 | TEPS 1898907.35\n",
            "Epoch 00089 | Time(s) 0.0056 | Loss 0.1634 | Accuracy 0.6740 | TEPS 1900463.33\n",
            "Epoch 00090 | Time(s) 0.0055 | Loss 0.2144 | Accuracy 0.6540 | TEPS 1902107.90\n",
            "Epoch 00091 | Time(s) 0.0055 | Loss 0.1342 | Accuracy 0.6700 | TEPS 1904309.56\n",
            "Epoch 00092 | Time(s) 0.0055 | Loss 0.1586 | Accuracy 0.6800 | TEPS 1902257.02\n",
            "Epoch 00093 | Time(s) 0.0056 | Loss 0.0768 | Accuracy 0.6840 | TEPS 1899344.45\n",
            "Epoch 00094 | Time(s) 0.0056 | Loss 0.1923 | Accuracy 0.6620 | TEPS 1901403.15\n",
            "Epoch 00095 | Time(s) 0.0055 | Loss 0.1585 | Accuracy 0.7080 | TEPS 1902093.63\n",
            "Epoch 00096 | Time(s) 0.0056 | Loss 0.1295 | Accuracy 0.7020 | TEPS 1900221.09\n",
            "Epoch 00097 | Time(s) 0.0056 | Loss 0.1085 | Accuracy 0.7020 | TEPS 1901956.09\n",
            "Epoch 00098 | Time(s) 0.0056 | Loss 0.2256 | Accuracy 0.6620 | TEPS 1896897.11\n",
            "Epoch 00099 | Time(s) 0.0056 | Loss 0.1428 | Accuracy 0.6560 | TEPS 1893463.83\n",
            "Epoch 00100 | Time(s) 0.0056 | Loss 0.1114 | Accuracy 0.6940 | TEPS 1894928.23\n",
            "Epoch 00101 | Time(s) 0.0056 | Loss 0.1961 | Accuracy 0.6660 | TEPS 1886327.04\n",
            "Epoch 00102 | Time(s) 0.0056 | Loss 0.1506 | Accuracy 0.6820 | TEPS 1887070.59\n",
            "Epoch 00103 | Time(s) 0.0056 | Loss 0.1110 | Accuracy 0.6620 | TEPS 1888682.36\n",
            "Epoch 00104 | Time(s) 0.0056 | Loss 0.1391 | Accuracy 0.6760 | TEPS 1889156.91\n",
            "Epoch 00105 | Time(s) 0.0056 | Loss 0.1166 | Accuracy 0.6780 | TEPS 1890297.83\n",
            "Epoch 00106 | Time(s) 0.0056 | Loss 0.1721 | Accuracy 0.7040 | TEPS 1891844.02\n",
            "Epoch 00107 | Time(s) 0.0056 | Loss 0.2058 | Accuracy 0.6820 | TEPS 1893776.45\n",
            "Epoch 00108 | Time(s) 0.0056 | Loss 0.1761 | Accuracy 0.6960 | TEPS 1889856.52\n",
            "Epoch 00109 | Time(s) 0.0056 | Loss 0.1222 | Accuracy 0.6800 | TEPS 1889373.91\n",
            "Epoch 00110 | Time(s) 0.0056 | Loss 0.1367 | Accuracy 0.6720 | TEPS 1889808.91\n",
            "Epoch 00111 | Time(s) 0.0056 | Loss 0.1252 | Accuracy 0.6960 | TEPS 1890936.95\n",
            "Epoch 00112 | Time(s) 0.0056 | Loss 0.1596 | Accuracy 0.6580 | TEPS 1890335.63\n",
            "Epoch 00113 | Time(s) 0.0056 | Loss 0.0857 | Accuracy 0.6660 | TEPS 1887550.91\n",
            "Epoch 00114 | Time(s) 0.0056 | Loss 0.1517 | Accuracy 0.6840 | TEPS 1889202.61\n",
            "Epoch 00115 | Time(s) 0.0056 | Loss 0.1698 | Accuracy 0.6300 | TEPS 1890844.66\n",
            "Epoch 00116 | Time(s) 0.0056 | Loss 0.1468 | Accuracy 0.6580 | TEPS 1892520.88\n",
            "Epoch 00117 | Time(s) 0.0057 | Loss 0.1159 | Accuracy 0.6480 | TEPS 1866725.44\n",
            "Epoch 00118 | Time(s) 0.0057 | Loss 0.1547 | Accuracy 0.6120 | TEPS 1864305.95\n",
            "Epoch 00119 | Time(s) 0.0057 | Loss 0.1560 | Accuracy 0.6700 | TEPS 1865880.45\n",
            "training complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yTTbNBFt2wn",
        "outputId": "c8fc8efa-c462-42a8-e664-0eb4327a9695"
      },
      "source": [
        "acc = get_accuracy(last_output[test_mask], labels_test)\n",
        "print(\"Test Accuracy {:.4f}\".format(acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.6790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80-x2gnCt2wn"
      },
      "source": [
        "Slightly better! Lets change the aggregation function. \n",
        "in the original GraphSAGE paper they found the LSTM and pool methods generally outperformed the mean and GCN aggregation across a range of datasets.\n",
        "Lets try the **pool** method (which refers to a max pool aggregator over the neighbourhood) and bump the number of hidden channels up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mhJL9KKt2wo"
      },
      "source": [
        "model = GraphSAGE(g, n_features, 128, n_classes, 2, 'pool', F.relu, 0.3)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=0.003, weight_decay=weight_decay\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Qj8gBCt2wo",
        "outputId": "c2ef9e33-18a5-42ec-899d-42f546922689"
      },
      "source": [
        "model, last_output, all_train_logits = train(model, optimizer, n_epochs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Time(s) 0.0190 | Loss 1.9694 | Accuracy 0.1220 | TEPS 554728.16\n",
            "Epoch 00001 | Time(s) 0.0147 | Loss 1.9235 | Accuracy 0.1300 | TEPS 718762.85\n",
            "Epoch 00002 | Time(s) 0.0132 | Loss 1.9324 | Accuracy 0.1460 | TEPS 801189.61\n",
            "Epoch 00003 | Time(s) 0.0124 | Loss 1.8544 | Accuracy 0.2440 | TEPS 851108.18\n",
            "Epoch 00004 | Time(s) 0.0122 | Loss 1.7394 | Accuracy 0.2260 | TEPS 866582.24\n",
            "Epoch 00005 | Time(s) 0.0118 | Loss 1.7740 | Accuracy 0.1660 | TEPS 892546.63\n",
            "Epoch 00006 | Time(s) 0.0116 | Loss 1.6723 | Accuracy 0.2180 | TEPS 912117.04\n",
            "Epoch 00007 | Time(s) 0.0114 | Loss 1.5531 | Accuracy 0.3140 | TEPS 925876.63\n",
            "Epoch 00008 | Time(s) 0.0116 | Loss 1.4408 | Accuracy 0.3940 | TEPS 913603.92\n",
            "Epoch 00009 | Time(s) 0.0114 | Loss 1.4260 | Accuracy 0.3620 | TEPS 925681.60\n",
            "Epoch 00010 | Time(s) 0.0111 | Loss 1.2600 | Accuracy 0.4380 | TEPS 949042.44\n",
            "Epoch 00011 | Time(s) 0.0109 | Loss 1.2367 | Accuracy 0.4500 | TEPS 970806.20\n",
            "Epoch 00012 | Time(s) 0.0107 | Loss 1.0003 | Accuracy 0.5120 | TEPS 988386.40\n",
            "Epoch 00013 | Time(s) 0.0105 | Loss 0.8699 | Accuracy 0.5500 | TEPS 1005964.24\n",
            "Epoch 00014 | Time(s) 0.0106 | Loss 0.7716 | Accuracy 0.6060 | TEPS 991719.96\n",
            "Epoch 00015 | Time(s) 0.0105 | Loss 0.6182 | Accuracy 0.6380 | TEPS 1003922.96\n",
            "Epoch 00016 | Time(s) 0.0104 | Loss 0.5728 | Accuracy 0.6320 | TEPS 1015765.64\n",
            "Epoch 00017 | Time(s) 0.0103 | Loss 0.4166 | Accuracy 0.6560 | TEPS 1027685.12\n",
            "Epoch 00018 | Time(s) 0.0102 | Loss 0.3794 | Accuracy 0.6540 | TEPS 1037475.20\n",
            "Epoch 00019 | Time(s) 0.0101 | Loss 0.3197 | Accuracy 0.7040 | TEPS 1047453.48\n",
            "Epoch 00020 | Time(s) 0.0100 | Loss 0.2862 | Accuracy 0.6980 | TEPS 1056176.58\n",
            "Epoch 00021 | Time(s) 0.0099 | Loss 0.3573 | Accuracy 0.5800 | TEPS 1064059.32\n",
            "Epoch 00022 | Time(s) 0.0099 | Loss 0.2076 | Accuracy 0.6800 | TEPS 1070899.27\n",
            "Epoch 00023 | Time(s) 0.0098 | Loss 0.1996 | Accuracy 0.7000 | TEPS 1078896.32\n",
            "Epoch 00024 | Time(s) 0.0097 | Loss 0.1119 | Accuracy 0.7280 | TEPS 1087976.50\n",
            "Epoch 00025 | Time(s) 0.0097 | Loss 0.1195 | Accuracy 0.7100 | TEPS 1092354.81\n",
            "Epoch 00026 | Time(s) 0.0096 | Loss 0.1279 | Accuracy 0.6900 | TEPS 1098818.82\n",
            "Epoch 00027 | Time(s) 0.0096 | Loss 0.1062 | Accuracy 0.7220 | TEPS 1104637.96\n",
            "Epoch 00028 | Time(s) 0.0095 | Loss 0.0550 | Accuracy 0.7320 | TEPS 1110887.51\n",
            "Epoch 00029 | Time(s) 0.0095 | Loss 0.0511 | Accuracy 0.7280 | TEPS 1114590.72\n",
            "Epoch 00030 | Time(s) 0.0094 | Loss 0.0397 | Accuracy 0.7300 | TEPS 1120114.96\n",
            "Epoch 00031 | Time(s) 0.0094 | Loss 0.0455 | Accuracy 0.7260 | TEPS 1125433.29\n",
            "Epoch 00032 | Time(s) 0.0094 | Loss 0.0146 | Accuracy 0.7360 | TEPS 1128321.78\n",
            "Epoch 00033 | Time(s) 0.0094 | Loss 0.0466 | Accuracy 0.6740 | TEPS 1128631.19\n",
            "Epoch 00034 | Time(s) 0.0095 | Loss 0.0456 | Accuracy 0.6760 | TEPS 1110764.50\n",
            "Epoch 00035 | Time(s) 0.0095 | Loss 0.0367 | Accuracy 0.7500 | TEPS 1114027.49\n",
            "Epoch 00036 | Time(s) 0.0094 | Loss 0.0154 | Accuracy 0.7580 | TEPS 1119100.73\n",
            "Epoch 00037 | Time(s) 0.0094 | Loss 0.0218 | Accuracy 0.7200 | TEPS 1117206.58\n",
            "Epoch 00038 | Time(s) 0.0094 | Loss 0.0882 | Accuracy 0.6980 | TEPS 1121637.14\n",
            "Epoch 00039 | Time(s) 0.0094 | Loss 0.0114 | Accuracy 0.7500 | TEPS 1124930.55\n",
            "Epoch 00040 | Time(s) 0.0094 | Loss 0.0249 | Accuracy 0.7460 | TEPS 1128203.31\n",
            "Epoch 00041 | Time(s) 0.0093 | Loss 0.0094 | Accuracy 0.7400 | TEPS 1131353.80\n",
            "Epoch 00042 | Time(s) 0.0094 | Loss 0.0129 | Accuracy 0.7520 | TEPS 1123543.15\n",
            "Epoch 00043 | Time(s) 0.0094 | Loss 0.0178 | Accuracy 0.7320 | TEPS 1127442.40\n",
            "Epoch 00044 | Time(s) 0.0093 | Loss 0.0226 | Accuracy 0.7240 | TEPS 1130857.92\n",
            "Epoch 00045 | Time(s) 0.0093 | Loss 0.0055 | Accuracy 0.7100 | TEPS 1134102.69\n",
            "Epoch 00046 | Time(s) 0.0093 | Loss 0.0126 | Accuracy 0.6840 | TEPS 1136722.44\n",
            "Epoch 00047 | Time(s) 0.0093 | Loss 0.0303 | Accuracy 0.7080 | TEPS 1139686.74\n",
            "Epoch 00048 | Time(s) 0.0092 | Loss 0.0118 | Accuracy 0.6880 | TEPS 1142327.44\n",
            "Epoch 00049 | Time(s) 0.0092 | Loss 0.0138 | Accuracy 0.7020 | TEPS 1145638.35\n",
            "Epoch 00050 | Time(s) 0.0092 | Loss 0.0159 | Accuracy 0.7060 | TEPS 1148310.57\n",
            "Epoch 00051 | Time(s) 0.0092 | Loss 0.0129 | Accuracy 0.7520 | TEPS 1144428.38\n",
            "Epoch 00052 | Time(s) 0.0092 | Loss 0.0047 | Accuracy 0.7340 | TEPS 1146261.17\n",
            "Epoch 00053 | Time(s) 0.0092 | Loss 0.0130 | Accuracy 0.7260 | TEPS 1148420.40\n",
            "Epoch 00054 | Time(s) 0.0092 | Loss 0.0151 | Accuracy 0.7280 | TEPS 1150427.84\n",
            "Epoch 00055 | Time(s) 0.0092 | Loss 0.0076 | Accuracy 0.7140 | TEPS 1152751.18\n",
            "Epoch 00056 | Time(s) 0.0091 | Loss 0.0162 | Accuracy 0.6600 | TEPS 1155102.35\n",
            "Epoch 00057 | Time(s) 0.0091 | Loss 0.0178 | Accuracy 0.6940 | TEPS 1157457.21\n",
            "Epoch 00058 | Time(s) 0.0091 | Loss 0.0036 | Accuracy 0.7500 | TEPS 1158852.33\n",
            "Epoch 00059 | Time(s) 0.0091 | Loss 0.0062 | Accuracy 0.7420 | TEPS 1154936.12\n",
            "Epoch 00060 | Time(s) 0.0091 | Loss 0.0030 | Accuracy 0.7400 | TEPS 1156892.02\n",
            "Epoch 00061 | Time(s) 0.0091 | Loss 0.0188 | Accuracy 0.7220 | TEPS 1156784.62\n",
            "Epoch 00062 | Time(s) 0.0091 | Loss 0.0145 | Accuracy 0.7240 | TEPS 1159118.58\n",
            "Epoch 00063 | Time(s) 0.0091 | Loss 0.0619 | Accuracy 0.7160 | TEPS 1159107.76\n",
            "Epoch 00064 | Time(s) 0.0091 | Loss 0.0083 | Accuracy 0.7160 | TEPS 1160980.76\n",
            "Epoch 00065 | Time(s) 0.0091 | Loss 0.0181 | Accuracy 0.6460 | TEPS 1158273.46\n",
            "Epoch 00066 | Time(s) 0.0091 | Loss 0.0883 | Accuracy 0.6320 | TEPS 1159872.16\n",
            "Epoch 00067 | Time(s) 0.0091 | Loss 0.0236 | Accuracy 0.7020 | TEPS 1161419.12\n",
            "Epoch 00068 | Time(s) 0.0091 | Loss 0.0085 | Accuracy 0.7060 | TEPS 1160448.78\n",
            "Epoch 00069 | Time(s) 0.0091 | Loss 0.2178 | Accuracy 0.6700 | TEPS 1161983.04\n",
            "Epoch 00070 | Time(s) 0.0091 | Loss 0.0076 | Accuracy 0.7440 | TEPS 1162641.88\n",
            "Epoch 00071 | Time(s) 0.0091 | Loss 0.0453 | Accuracy 0.6860 | TEPS 1164089.82\n",
            "Epoch 00072 | Time(s) 0.0091 | Loss 0.1517 | Accuracy 0.6220 | TEPS 1165453.64\n",
            "Epoch 00073 | Time(s) 0.0090 | Loss 0.0250 | Accuracy 0.6520 | TEPS 1166979.84\n",
            "Epoch 00074 | Time(s) 0.0090 | Loss 0.2009 | Accuracy 0.5960 | TEPS 1168784.21\n",
            "Epoch 00075 | Time(s) 0.0090 | Loss 0.0156 | Accuracy 0.7220 | TEPS 1170517.96\n",
            "Epoch 00076 | Time(s) 0.0090 | Loss 0.0352 | Accuracy 0.7160 | TEPS 1169748.13\n",
            "Epoch 00077 | Time(s) 0.0090 | Loss 0.1353 | Accuracy 0.6800 | TEPS 1170855.05\n",
            "Epoch 00078 | Time(s) 0.0090 | Loss 0.0453 | Accuracy 0.7280 | TEPS 1172378.66\n",
            "Epoch 00079 | Time(s) 0.0090 | Loss 0.2196 | Accuracy 0.6260 | TEPS 1174004.57\n",
            "Epoch 00080 | Time(s) 0.0090 | Loss 0.1302 | Accuracy 0.6580 | TEPS 1175398.19\n",
            "Epoch 00081 | Time(s) 0.0090 | Loss 0.0385 | Accuracy 0.7020 | TEPS 1176950.60\n",
            "Epoch 00082 | Time(s) 0.0090 | Loss 0.0427 | Accuracy 0.6980 | TEPS 1174174.35\n",
            "Epoch 00083 | Time(s) 0.0090 | Loss 0.0154 | Accuracy 0.7080 | TEPS 1173724.27\n",
            "Epoch 00084 | Time(s) 0.0090 | Loss 0.0266 | Accuracy 0.7040 | TEPS 1173334.49\n",
            "Epoch 00085 | Time(s) 0.0090 | Loss 0.0326 | Accuracy 0.6900 | TEPS 1170482.33\n",
            "Epoch 00086 | Time(s) 0.0090 | Loss 0.0288 | Accuracy 0.6740 | TEPS 1169969.45\n",
            "Epoch 00087 | Time(s) 0.0090 | Loss 0.0338 | Accuracy 0.6440 | TEPS 1171399.30\n",
            "Epoch 00088 | Time(s) 0.0090 | Loss 0.0342 | Accuracy 0.6540 | TEPS 1172626.94\n",
            "Epoch 00089 | Time(s) 0.0090 | Loss 0.0297 | Accuracy 0.6620 | TEPS 1173567.75\n",
            "Epoch 00090 | Time(s) 0.0090 | Loss 0.0148 | Accuracy 0.6920 | TEPS 1175269.43\n",
            "Epoch 00091 | Time(s) 0.0090 | Loss 0.0128 | Accuracy 0.7060 | TEPS 1176495.30\n",
            "Epoch 00092 | Time(s) 0.0090 | Loss 0.0076 | Accuracy 0.7200 | TEPS 1177650.79\n",
            "Epoch 00093 | Time(s) 0.0090 | Loss 0.0052 | Accuracy 0.7240 | TEPS 1175727.22\n",
            "Epoch 00094 | Time(s) 0.0090 | Loss 0.0070 | Accuracy 0.7280 | TEPS 1176484.13\n",
            "Epoch 00095 | Time(s) 0.0090 | Loss 0.0128 | Accuracy 0.7120 | TEPS 1177756.28\n",
            "Epoch 00096 | Time(s) 0.0090 | Loss 0.0100 | Accuracy 0.7340 | TEPS 1179214.01\n",
            "Epoch 00097 | Time(s) 0.0089 | Loss 0.0096 | Accuracy 0.7360 | TEPS 1180413.58\n",
            "Epoch 00098 | Time(s) 0.0089 | Loss 0.0090 | Accuracy 0.7280 | TEPS 1181346.71\n",
            "Epoch 00099 | Time(s) 0.0089 | Loss 0.0096 | Accuracy 0.7360 | TEPS 1180414.27\n",
            "Epoch 00100 | Time(s) 0.0090 | Loss 0.0072 | Accuracy 0.7480 | TEPS 1179291.14\n",
            "Epoch 00101 | Time(s) 0.0089 | Loss 0.0103 | Accuracy 0.7440 | TEPS 1180467.09\n",
            "Epoch 00102 | Time(s) 0.0090 | Loss 0.0093 | Accuracy 0.7460 | TEPS 1179117.83\n",
            "Epoch 00103 | Time(s) 0.0089 | Loss 0.0093 | Accuracy 0.7480 | TEPS 1180203.19\n",
            "Epoch 00104 | Time(s) 0.0089 | Loss 0.0064 | Accuracy 0.7540 | TEPS 1181432.23\n",
            "Epoch 00105 | Time(s) 0.0089 | Loss 0.0097 | Accuracy 0.7540 | TEPS 1182560.71\n",
            "Epoch 00106 | Time(s) 0.0089 | Loss 0.0048 | Accuracy 0.7580 | TEPS 1183544.52\n",
            "Epoch 00107 | Time(s) 0.0089 | Loss 0.0044 | Accuracy 0.7360 | TEPS 1184575.37\n",
            "Epoch 00108 | Time(s) 0.0089 | Loss 0.0064 | Accuracy 0.7320 | TEPS 1185535.76\n",
            "Epoch 00109 | Time(s) 0.0089 | Loss 0.0067 | Accuracy 0.7560 | TEPS 1184367.57\n",
            "Epoch 00110 | Time(s) 0.0089 | Loss 0.0032 | Accuracy 0.7620 | TEPS 1182045.89\n",
            "Epoch 00111 | Time(s) 0.0089 | Loss 0.0067 | Accuracy 0.7360 | TEPS 1182729.10\n",
            "Epoch 00112 | Time(s) 0.0089 | Loss 0.0044 | Accuracy 0.7460 | TEPS 1183800.83\n",
            "Epoch 00113 | Time(s) 0.0089 | Loss 0.0044 | Accuracy 0.7440 | TEPS 1184785.29\n",
            "Epoch 00114 | Time(s) 0.0089 | Loss 0.0151 | Accuracy 0.7280 | TEPS 1185843.15\n",
            "Epoch 00115 | Time(s) 0.0089 | Loss 0.0037 | Accuracy 0.7320 | TEPS 1186728.02\n",
            "Epoch 00116 | Time(s) 0.0089 | Loss 0.0038 | Accuracy 0.7400 | TEPS 1184533.64\n",
            "Epoch 00117 | Time(s) 0.0089 | Loss 0.0036 | Accuracy 0.7340 | TEPS 1183842.65\n",
            "Epoch 00118 | Time(s) 0.0089 | Loss 0.0031 | Accuracy 0.7400 | TEPS 1184789.69\n",
            "Epoch 00119 | Time(s) 0.0089 | Loss 0.0048 | Accuracy 0.7060 | TEPS 1185531.66\n",
            "training complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJg8VMvct2wp",
        "outputId": "b7bcf12f-18dc-4e78-aac1-7493b1ffdff5"
      },
      "source": [
        "acc = get_accuracy(last_output[test_mask], labels_test)\n",
        "print(\"Test Accuracy {:.4f}\".format(acc))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.7500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bArleowDt2wq"
      },
      "source": [
        "And finally the **LSTM** aggregation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQXlnGimt2wq"
      },
      "source": [
        "model = GraphSAGE(g, n_features, 128, n_classes, 2, 'lstm', F.relu, 0.1)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=0.003, weight_decay=weight_decay\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXPwp6JGt2wq",
        "outputId": "8db8596f-5439-46c6-9386-58ab710bba55"
      },
      "source": [
        "model, last_output, all_train_logits = train(model, optimizer, n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Time(s) 0.4972 | Loss 1.9492 | Accuracy 0.1060 | TEPS 21232.81\n",
            "Epoch 00001 | Time(s) 0.4820 | Loss 1.9285 | Accuracy 0.0900 | TEPS 21898.38\n",
            "Epoch 00002 | Time(s) 0.4774 | Loss 1.9310 | Accuracy 0.2720 | TEPS 22113.68\n",
            "Epoch 00003 | Time(s) 0.4725 | Loss 1.8781 | Accuracy 0.1520 | TEPS 22340.10\n",
            "Epoch 00004 | Time(s) 0.4701 | Loss 1.8380 | Accuracy 0.1400 | TEPS 22453.59\n",
            "Epoch 00005 | Time(s) 0.4686 | Loss 1.7596 | Accuracy 0.2060 | TEPS 22525.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoOwF4Ogt2wq"
      },
      "source": [
        "acc = get_accuracy(last_output[test_mask], labels_test)\n",
        "print(\"Test Accuracy {:.4f}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBiQ3rrtt2wr"
      },
      "source": [
        "Not bad! See how high you can get the accuracy with some tweaking. \n",
        "Compare against the state-of-the-art here: https://paperswithcode.com/sota/node-classification-on-cora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te521FPqt2wr"
      },
      "source": [
        "We can plot an animation of the predictions during training (although we are limited to 2D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QNhB3OUt2wr"
      },
      "source": [
        "# one colour for each class\n",
        "colors = ['red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink']\n",
        "# to keep the graph small lets only consider training nodes\n",
        "train_nodes = train_mask.cpu().numpy()\n",
        "labels_train = labels[train_mask]\n",
        "non_train = np.ones(len(train_nodes))\n",
        "non_train[train_nodes] = 0\n",
        "non_train = np.where(non_train)[0]\n",
        "if torch.cuda.is_available():\n",
        "  nx_g = model.g.cpu().to_networkx()\n",
        "else:\n",
        "  nx_g = model.g.to_networkx()\n",
        "nx_g.remove_nodes_from(non_train)\n",
        "rn_nodes = range(nx_g.number_of_nodes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3C5bvfft2wr"
      },
      "source": [
        "def draw_epoch(i):\n",
        "    current_colors = []\n",
        "    if torch.cuda.is_available():\n",
        "        logits = all_train_logits[i].detach().cpu().numpy()\n",
        "    else:\n",
        "        logits = all_train_logits[i].detach().numpy()\n",
        "        \n",
        "    max_ix = logits.argmax(axis=1)\n",
        "    \n",
        "    # choose x, y position based on the magnitude of their highest \n",
        "    #min_ix = max_ix - 1\n",
        "    #pos = {n: [logits[n, max_ix[n]], logits[n, min_ix[n]]] for n in rn_nodes}\n",
        "    #node_size = 100\n",
        "    # x=node_index, y = certainty, color=class \n",
        "    #pos = {n: [n, logits[n, max_ix[n]]] for n in rn_nodes}\n",
        "    #node_size = 100\n",
        "    \n",
        "    # x=node_index, y = class, size = certainty\n",
        "    pos = {n: [n, max_ix[n]] for n in rn_nodes}\n",
        "    node_size = logits.max(axis=1) * 100\n",
        "    \n",
        "    # cols = [colors[max_ix[n]] for n in rn_nodes]\n",
        "    # use real label for color    \n",
        "    cols = [colors[labels_train[n]] for n in rn_nodes]\n",
        "    \n",
        "    ax.cla()\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Epoch: %d' % i)\n",
        "    nx.draw_networkx(nx_g, pos, node_color=cols,\n",
        "            with_labels=True, node_size=node_size, ax=ax, \n",
        "            edge_color='purple', arrows=False, alpha=0.6)\n",
        "\n",
        "fig = plt.figure(dpi=100)\n",
        "fig.clf()\n",
        "ax = fig.subplots()\n",
        "draw_epoch(0)  # draw the prediction of the first epoch\n",
        "plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s0o1A8xt2ws"
      },
      "source": [
        "ani = animation.FuncAnimation(fig, draw_epoch, frames=len(all_train_logits), interval=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngxlIRrpt2ws",
        "scrolled": false
      },
      "source": [
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo2w5BqPt2xI"
      },
      "source": [
        "Note how the separation of the nodes into classes improves with more training epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InkJXgwht2xI"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  logits = all_train_logits[0].cpu().detach().numpy()\n",
        "else:\n",
        "  logits = all_train_logits[0].detach().numpy()\n",
        "logits.max(axis=1)\n",
        "# logits.argmax(axis=1)\n",
        "# logits"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}