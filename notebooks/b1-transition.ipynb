{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "b1-transition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gnn_tutorial",
      "language": "python",
      "name": "gnn_tutorial"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekEWr9kScxL1"
      },
      "source": [
        "# The Role of Recurrent Transitions in RGNNs for Graph Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtRaKH6McxL3"
      },
      "source": [
        "Social networks represent a rich source of graph data, and due to the popularity of social networking applications, accurate user and community classifications have become exceedingly important for the purpose of analysis, marketing, and influencing. \n",
        "\n",
        "In this example, we look at how the recurrent application of a transition function aids in making predictions on the graph domain, namely, in graph classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOwR4kBLcxL4"
      },
      "source": [
        "## Setup\n",
        "Here we load required libraries, define paths to data, and define some helper functions. **Feel free to skip this section.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddJ72XKZcxL4",
        "outputId": "31b35db5-a24a-4f90-c927-3c2066e1d6bc"
      },
      "source": [
        "!pip install dgl\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import networkx as nx\n",
        "import itertools\n",
        "\n",
        "from IPython.display import HTML\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dgl\n",
        "from dgl.nn.pytorch import GraphConv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import manifold"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c4/ce24841375cf4393787dbf9a645e271c19a03d2d9a0e5770b08ba76bcfde/dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDs7nAgicxL7"
      },
      "source": [
        "# Helper function to visualize stargazer dataset graphs\n",
        "def plot_stargazer_graph(G, label=None, print_info=False, labels=True):\n",
        "    # Print info with graph if we want\n",
        "    if print_info:  \n",
        "        print(f\"{G.number_of_nodes()} vertices\")\n",
        "        print(f\"{G.number_of_edges()} edges\")\n",
        "        \n",
        "    # Grey if no label, pink if 0, green if 1\n",
        "    col = [[0.7, 0.7, 0.7]]\n",
        "    if label is not None:\n",
        "        if label == 0:\n",
        "            col = [[0.6, 0, 0.6]]\n",
        "        elif label == 1:\n",
        "            col = [[0, 0.5, 0]]\n",
        "    \n",
        "    # Visualise as undirected\n",
        "    nx_G = G.to_networkx().to_undirected()\n",
        "    \n",
        "    # Use the recommended Kamada-Kawaii layout for arbitrary graphs\n",
        "    pos = nx.kamada_kawai_layout(nx_G)\n",
        "    nx.draw(nx_G, pos, with_labels=labels, node_color=col)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBIYJPJucxL8"
      },
      "source": [
        "# Helper function to visualize the webmluser graph\n",
        "def plot_webmluser_graph(G):    \n",
        "    # Visualise as undirected\n",
        "    nx_G = G.to_networkx().to_undirected()\n",
        "    \n",
        "    # Use the recommended Kamada-Kawaii layout for arbitrary graphs\n",
        "    pos = nx.kamada_kawai_layout(nx_G)\n",
        "    nx.draw(nx_G, pos, node_color=[[0.7, 0.7, 0.7]])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOVdym2rcxL8"
      },
      "source": [
        "# Helper for getting batches from a dataset\n",
        "def get_batches(xs, ys, batch_size=16):\n",
        "    # How many batches is there of given size for this dataset\n",
        "    num = len(xs)\n",
        "    num_batches = math.ceil(num / batch_size)\n",
        "    \n",
        "    # Go through and get all batches\n",
        "    batches_x = []\n",
        "    batches_y = []\n",
        "    \n",
        "    # Get all batches in memory\n",
        "    for i in range(num_batches - 1):\n",
        "        sidx = batch_size * i\n",
        "        fidx = batch_size * (i + 1)\n",
        "        fidx = min(fidx, num)\n",
        "        batches_x.append(xs[sidx:fidx])\n",
        "        batches_y.append(ys[sidx:fidx])\n",
        "        \n",
        "    return batches_x, batches_y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHv25MepcxL9"
      },
      "source": [
        "## Datasets\n",
        "We'll introduce two distinct kinds of graph datasets through the following examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6SeGWsqcxL9"
      },
      "source": [
        "\n",
        "### GitHub Stargazers Dataset\n",
        "12,725 graphs of undirected developer social networks. Again, vertices are users who starred popular machine learning and web development repositories (with at least 10 stars), and edges are mutual follower relationships. The task is to classify each graph as a machine learning, or web development social network.\n",
        "\n",
        "**In this dataset, each training sample is a distinct graph.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9_19GOFcxL-",
        "outputId": "01928911-afec-4d7f-9e5d-40cde939ff3f"
      },
      "source": [
        "# check whether you have cloned the repo from Github and have the data locally, \n",
        "# or running on colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if not IN_COLAB:\n",
        "  # set this path to where you have the data locally\n",
        "  fp_data = \"./datasets\"\n",
        "else:\n",
        "  !git clone https://github.com/zephyros-solutions/gnn-tutorial.git\n",
        "  fp_data = \"./gnn-tutorial/notebooks/datasets\"\n",
        "\n",
        "fp_data_stargazer = f\"{fp_data}/github_stargazers\"\n",
        "fp_data_webmluser = f\"{fp_data}/git_web_ml\"\n",
        "\n",
        "# Load the ground truth labels. Every element here is a \n",
        "# [graph index, label] pair\n",
        "stargazer_labels = np.loadtxt(f\"{fp_data_stargazer}/git_target.csv\", \\\n",
        "                                      delimiter=\",\", skiprows=1)\n",
        "\n",
        "    \n",
        "# Adopting the accepted ML notation of input data (x) and labels (y)\n",
        "stargazer_y = stargazer_labels[:,1].astype(int)\n",
        "\n",
        "print(stargazer_y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gnn-tutorial'...\n",
            "remote: Enumerating objects: 198, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 198 (delta 34), reused 72 (delta 24), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (198/198), 100.87 MiB | 15.11 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "[0 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRGLyHEdcxL-"
      },
      "source": [
        "# Then we'll load the edges (defined against vertex ids) - note that there\n",
        "# are no node or edge features, so predictions are made purely based on \n",
        "# undirected graph properties\n",
        "with open(f\"{fp_data_stargazer}/git_edges.json\") as f:\n",
        "  stargazer_edges = json.load(f)\n",
        "\n",
        "# print(stargazer_edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NjxUgpmcxL-"
      },
      "source": [
        "# Every item in this dictionary are the edges for a single graph, where each \n",
        "# edge is defined as a [vertex index, vertex index] pair. Here we show the \n",
        "# first 10 edges from graph 19\n",
        "stargazer_edges['19'][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QadphseacxL_"
      },
      "source": [
        "# This function takes the data loaded in the stargazer dataset form and turns \n",
        "# it into a DGL graph\n",
        "def make_stargazer_graph(index):\n",
        "    # Define every edge by its source and destination vertex ids\n",
        "    edges = np.array(stargazer_edges[f\"{index}\"])\n",
        "    src = edges[:,0]\n",
        "    dst = edges[:,1]\n",
        "    \n",
        "    # Edges are directional in DGL; Make them undirectional by making them \n",
        "    # go both ways\n",
        "    u = np.concatenate([src, dst])\n",
        "    v = np.concatenate([dst, src])\n",
        "    \n",
        "    # Construct a DGLGraph\n",
        "    return dgl.graph((u, v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvWcfkZycxMA"
      },
      "source": [
        "# We can use this data to build each of the 12725 graphs as a single DGL\n",
        "# graph object, thus forming our graph dataset\n",
        "stargazer_x = []\n",
        "for i in range(12725):\n",
        "    G = make_stargazer_graph(i)\n",
        "    stargazer_x.append(G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypf3Ml1EcxMA"
      },
      "source": [
        "# Lets look at one of our graphs (pink for class 0, green for class 1)\n",
        "plot_stargazer_graph(stargazer_x[19], label=stargazer_y[19], print_info=True, labels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8tj1SqOcxMB"
      },
      "source": [
        "# Lets look at another - noting that the structure alone will be the inputs \n",
        "# to the GNN (no vertex or edge features), and that this will be sufficient\n",
        "# to determine if the social network group is a ML developer group or \n",
        "# a Web developer group\n",
        "plot_stargazer_graph(stargazer_x[2], label=stargazer_y[2], print_info=True, labels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDQUU1OTcxMB"
      },
      "source": [
        "### GitHub Web or Machine Learning Dataset\n",
        "One single graph representing the GitHub social network: vertices are users (who have starred at least 10 repositories) and edges represent if they mutually follow eachother. Each node has input features derived from profile data (location, repositories starred, employer and e-mail address), and belongs to one of two classes (derived from the user's job title): a web developer or machine learning developer. \n",
        "\n",
        "**Note that using a single graph represents a departure from traditional supervised machine learning, where we typically use hundreds or thousands of training instances**. In this application, it is perhaps easier to consider the **vertices** of the graph as training instances. Indeed, we would reserve some vertices for training and some for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_kjcKsXcxMC"
      },
      "source": [
        "# Lets start by loading the vertices with their ground truth labels \n",
        "# and names attached. Every element is a [vertex index, name, label] triplet\n",
        "webmluser_labels = np.loadtxt(f\"{fp_data_webmluser}/musae_git_target.csv\", \\\n",
        "                                  delimiter=\",\", skiprows=1, usecols=[0,2])\n",
        "webmluser_y = webmluser_labels[:,1].astype(int)\n",
        "print(webmluser_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2SULiFLcxMC"
      },
      "source": [
        "# Now load the associated vertex features\n",
        "with open(f\"{fp_data_webmluser}/musae_git_features.json\") as f:\n",
        "    webmluser_features = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-mYCxnncxMC"
      },
      "source": [
        "# There are 37700 vertices here, with each index keying into the features\n",
        "# dictionary we just loaded. Here is an example of one vertex's features\n",
        "print(webmluser_features['9001'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uy8Ir3VcxMC"
      },
      "source": [
        "# Then we'll load the edges (defined against vertex id pairs)\n",
        "webmluser_edges = np.loadtxt(f\"{fp_data_webmluser}/musae_git_edges.csv\", \\\n",
        "                                  delimiter=\",\", skiprows=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txQxVt74cxMD"
      },
      "source": [
        "print(type(webmluser_edges[0,0]))\n",
        "# We only need to make one graph here, and that is our 'dataset'\n",
        "def make_webmluser_graph():\n",
        "    # Define every edge by its source and destination vertex ids\n",
        "    src = webmluser_edges[:,0].astype(int)\n",
        "    dst = webmluser_edges[:,1].astype(int)\n",
        "    \n",
        "    # Edges are directional in DGL; Make them undirectional by making them \n",
        "    # go both ways\n",
        "    u = np.concatenate([src, dst])\n",
        "    v = np.concatenate([dst, src])\n",
        "    \n",
        "    # Construct a DGLGraph\n",
        "    G = dgl.graph((u, v))\n",
        "    \n",
        "    return G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXX34GKncxMD"
      },
      "source": [
        "# Build the graph from the data - note that this is a big graph\n",
        "webmluser_x = make_webmluser_graph()\n",
        "print(f\"{webmluser_x.number_of_nodes()} nodes\")\n",
        "print(f\"{webmluser_x.number_of_edges()} edges\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X2BvJSPcxMD"
      },
      "source": [
        "# For the purpose of this example, we will consider only the\n",
        "# Stargazer's Dataset, but its good to be aware of different kinds of \n",
        "# datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW-ebdKCcxME"
      },
      "source": [
        "## Architectures\n",
        "True RGNNs are seldom used in contemporary GNN work. RGNNs require a transition function to be repeatedly applied until convergence of the hidden states - an expensive computation. Instead of use true RGNNs here, we'll use GCNs with varying numbers of layers, thus mimicking the repeated application of a transition function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktVTO8aycxME"
      },
      "source": [
        "# We want to experiment with the amount of computation required in the \n",
        "# forward pass, so we'll build various GNNs here, each with different \n",
        "# numbers of transition function layers\n",
        "class StargazerGNN(nn.Module):\n",
        "    def __init__(self, num_hidden_features):\n",
        "        super().__init__()\n",
        "        \n",
        "        # We'll apply some number of spatial convolutions / message passing \n",
        "        # RGNNs\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(len(num_hidden_features) - 1):\n",
        "            self.convs.append(GraphConv(num_hidden_features[i], \n",
        "                                        num_hidden_features[i+1]))\n",
        "            \n",
        "        # Classify out to one of two classes\n",
        "        self.classify = nn.Linear(num_hidden_features[-1], 2)\n",
        "\n",
        "    def forward(self, g):\n",
        "        # Start with just the degree as a feature\n",
        "        h = g.in_degrees().view(-1, 1).float()\n",
        "        for i in range(len(self.convs)):\n",
        "            h = self.convs[i](g, h)\n",
        "            h = torch.relu(h)\n",
        "            \n",
        "        # Calculate graph representation by averaging all the node \n",
        "        # representations, thus making a graph representation\n",
        "        g.ndata['h'] = h\n",
        "        hg = dgl.mean_nodes(g, 'h')\n",
        "        output = self.classify(hg)\n",
        "        return output\n",
        "\n",
        "    def hidden(self, g):\n",
        "        # Start with just the degree as a feature\n",
        "        h = g.in_degrees().view(-1, 1).float()\n",
        "        for i in range(len(self.convs)):\n",
        "            h = self.convs[i](g, h)\n",
        "            h = torch.relu(h)\n",
        "        \n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZdCmStecxME"
      },
      "source": [
        "## Experiments\n",
        "Now we'll perform our experiments, we want to create GNNs which use learned transition functions of increasing complexity. \n",
        "\n",
        "We'll start by defining a training and testing procedure, and then we'll produce and discuss some results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHOHC9g8cxMF"
      },
      "source": [
        "### Training and Testing\n",
        "We'll use a learning rate of 0.001, MSE Loss in our loss function, and a batch size of 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjwNiEP2cxMF"
      },
      "source": [
        "# The following operations will train the GNN\n",
        "def train_stargazer_gnn(gnn, num_epochs):\n",
        "    loss_func = nn.MSELoss()\n",
        "    opt = torch.optim.Adam(itertools.chain(gnn.parameters()), \n",
        "                           lr=0.001)\n",
        "\n",
        "    # Get the batches to work with\n",
        "    num_train = 8096\n",
        "    batches_x, batches_y = get_batches(stargazer_x[:num_train], \\\n",
        "                                       stargazer_y[:num_train], \\\n",
        "                                       batch_size=64)\n",
        "\n",
        "    # Run every batch in every epoch\n",
        "    epoch_losses = []\n",
        "    for epoch_index in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_index in range(len(batches_x)):\n",
        "            # Get the batch of interest\n",
        "            batch_x = batches_x[batch_index]\n",
        "            batch_y = batches_y[batch_index]\n",
        "\n",
        "            # Calculate an output for each graph\n",
        "            x = dgl.batch(batch_x)\n",
        "            y_hat = gnn(x)\n",
        "\n",
        "            # And compare to the true value\n",
        "            y = F.one_hot(torch.tensor(batch_y), num_classes=2).float()\n",
        "\n",
        "            #print(y_hat)\n",
        "            #print(y)\n",
        "\n",
        "            # Calculate loss \n",
        "            loss = loss_func(y_hat, y)\n",
        "\n",
        "            # Calculate loss and perform gradient descent step accordingly\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        epoch_loss /= len(batches_x)\n",
        "        print(f\"Epoch {epoch_index}: loss {epoch_loss}\")\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        \n",
        "    # Return the trained architecture and loss\n",
        "    return epoch_losses\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx3WACqWcxMF"
      },
      "source": [
        "# The following operation will test the GNN, returning hidden representations and \n",
        "# predictions\n",
        "def test_stargazer_gnn(gnn):\n",
        "    num_train = 8096\n",
        "    num_test = 2048\n",
        "\n",
        "    batches_x, batches_y = get_batches(stargazer_x[num_train:num_train+num_test],\\\n",
        "                                       stargazer_y[num_train:num_train+num_test],\\\n",
        "                                       batch_size=1)\n",
        "\n",
        "    num_correct = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    hidden = []\n",
        "    for batch_index in range(len(batches_x)):\n",
        "        # Get the batch of interest\n",
        "        batch_x = batches_x[batch_index]\n",
        "        batch_y = batches_y[batch_index]\n",
        "\n",
        "        # Calculate an output for each graph\n",
        "        x = dgl.batch(batch_x)\n",
        "        y_hat = gnn(x)\n",
        "        y_hat = torch.argmax(y_hat, dim=1).detach().numpy()\n",
        "\n",
        "        # Also get some hidden representations for plotting\n",
        "        h = torch.mean(gnn.hidden(x), dim=0).detach().numpy()\n",
        "        hidden.append(h)\n",
        "\n",
        "        # Take note of everything\n",
        "        y_pred.extend(y_hat)\n",
        "        y_true.extend(batch_y)\n",
        "    hidden = np.array(hidden)\n",
        "    \n",
        "    return hidden, y_true, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWwWNt6gcxMG"
      },
      "source": [
        "As en example, lets now use these functions to train and test a Stargazer GNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57hLApxfcxMG"
      },
      "source": [
        "stargazer_gnn_example = StargazerGNN([1, 16, 16, 16, 16, 16, 16, 16])\n",
        "epoch_losses = train_stargazer_gnn(stargazer_gnn_example, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sQjJAnqcxMH"
      },
      "source": [
        "hidden, y_true, y_pred = test_stargazer_gnn(stargazer_gnn_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Ph6xKVcxMH"
      },
      "source": [
        "print(metrics.classification_report(y_true, y_pred, \\\n",
        "                                    target_names=['web', 'ml']))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
        "print(f\"AUC\\t{metrics.auc(fpr, tpr)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqYtNJsEcxMH"
      },
      "source": [
        "# Plot a TSNE fit on the features\n",
        "tsne = manifold.TSNE(n_components=2)\n",
        "tsne_embedded = tsne.fit_transform(hidden)\n",
        "tsne_embedded.shape\n",
        "plt.scatter(tsne_embedded[:,0], tsne_embedded[:,1], \\\n",
        "            marker='.', c=y_true, \n",
        "            cmap=matplotlib.colors.ListedColormap(['purple', 'green']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bua11IK7cxMH"
      },
      "source": [
        "As expected, the loss decreases (with ultimately diminishing returns) over the 8 epochs. Ultimately the features become more separable, as shown by the TSNE plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VJ8px-gcxMI"
      },
      "source": [
        "### Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83KPDZUucxMI"
      },
      "source": [
        "Now lets wrap all this up into one function so we can test multiple architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUU0bf-lcxMI"
      },
      "source": [
        "def train_test_architecture(num_hidden_features, num_epochs, plot_loss=False):\n",
        "    # Make architecture and train\n",
        "    gnn_run = StargazerGNN(num_hidden_features)\n",
        "    epoch_losses = train_stargazer_gnn(gnn_run, num_epochs)\n",
        "    \n",
        "    if num_epochs > 0 and plot_loss:\n",
        "        plt.plot(range(num_epochs), epoch_losses)\n",
        "        plt.ylim((0, 1.0))\n",
        "        plt.title(\"Loss during training\")\n",
        "        plt.show()\n",
        "    \n",
        "    # Test and return hidden representations plus predictions\n",
        "    hidden, y_true, y_pred = test_stargazer_gnn(gnn_run)\n",
        "    \n",
        "    # Print performance metrics\n",
        "    print(metrics.classification_report(y_true, y_pred, \\\n",
        "                                        target_names=['web', 'ml']))\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
        "    print(f\"AUC\\t{metrics.auc(fpr, tpr)}\")\n",
        "    \n",
        "    # Plot TSNE visualization\n",
        "    tsne = manifold.TSNE(n_components=2)\n",
        "    tsne_embedded = tsne.fit_transform(hidden)\n",
        "    tsne_embedded.shape\n",
        "    plt.scatter(tsne_embedded[:,0], tsne_embedded[:,1], \\\n",
        "                marker='.', c=y_true, \n",
        "                cmap=matplotlib.colors.ListedColormap(['purple', 'green']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-hK18g4cxMI"
      },
      "source": [
        "Now lets conduct some experiments - we keep the number of epochs consistent and vary the number of layers in the transition function. This is similar to the examples ran in section B.1 in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHK4Z8_EcxMI"
      },
      "source": [
        "train_test_architecture([1, 16], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XgX-TsTcxMJ"
      },
      "source": [
        "train_test_architecture([1, 16, 16], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kr5GU0WcxMJ"
      },
      "source": [
        "train_test_architecture([1, 16, 16, 16, 16], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEUh0mOucxMJ"
      },
      "source": [
        "train_test_architecture([1, 16, 16, 16, 16, 16, 16, 16], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM1lT74acxMJ"
      },
      "source": [
        "As expected, successive transition functions result in more discriminative features being calculated, thus resulting in a more discriminative final representation of the graph (analagous to more convolutional layers in a CNN). \n",
        "\n",
        "Note that this notebook is just here for demonstration - the actual results here display significant variance and may vary with successive repeats of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFcI9fbScxMK",
        "scrolled": true
      },
      "source": [
        "train_test_architecture([1, 32, 32, 32, 32, 32, 32, 32], 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsuI56rgcxMK",
        "scrolled": true
      },
      "source": [
        "print(hidden.shape)\n",
        "# print(hidden)\n",
        "tsne_embedded = tsne.fit_transform(hidden)\n",
        "print(tsne_embedded.shape)\n",
        "print(tsne_embedded[:10,])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}